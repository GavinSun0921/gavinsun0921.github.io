<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告 | Gavin Sun · Spatial Intelligence</title><meta name=keywords content="Diffusion,DPM,Computer Vision,Low-level Vision,Deep Learning"><meta name=description content="Image super-resolution with conditional diffusion model."><meta name=author content><link rel=canonical href=https://gavinsun0921.github.io/posts/fast-paper-reading-02/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://gavinsun0921.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gavinsun0921.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gavinsun0921.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gavinsun0921.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gavinsun0921.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gavinsun0921.github.io/posts/fast-paper-reading-02/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredTags:["script","noscript","style","textarea","pre"],throwOnError:!1})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-18P7N9RZDS"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-18P7N9RZDS")</script><meta property="og:url" content="https://gavinsun0921.github.io/posts/fast-paper-reading-02/"><meta property="og:site_name" content="Gavin Sun · Spatial Intelligence"><meta property="og:title" content="[T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告"><meta property="og:description" content="Image super-resolution with conditional diffusion model."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-05T00:00:00+00:00"><meta property="article:tag" content="Diffusion"><meta property="article:tag" content="DPM"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Low-Level Vision"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="[T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告"><meta name=twitter:description content="Image super-resolution with conditional diffusion model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gavinsun0921.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告","item":"https://gavinsun0921.github.io/posts/fast-paper-reading-02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告","name":"[T-PAMI\u002723] Image Super-Resolution via Iterative Refinement 阅读报告","description":"Image super-resolution with conditional diffusion model.","keywords":["Diffusion","DPM","Computer Vision","Low-level Vision","Deep Learning"],"articleBody":" This paper is published in TPAMI 2023. Overview Problem In the field of image super-resolution, existing approaches often suffer from various limitations; e.g., autoregressive models are prohibitively expensive for high-resolution image generation, Normalizing Flows (NFs) and variational autoencoders (VAEs) often yield sub-optimal sample quality, and GANs require carefully designed regularization and optimization tricks to tame optimization instability and model collapse. Solution Present SR3, an approach to image super-resolution via repeated refinement based on DDPM. Results The high-frequency information of the image can be well resored compared to other methods. Despite mediocre performance in SSIM and PSNR metrics, visualization and consistency are good. Related Work Diffusion Probabilistic Models I’ve written a blog about diffusion probabilistic models (DPM). It has the derivation of the basic formulas of the DPM as well as a simple code implementation.\nA Breif Exploration to Diffusion Probabilistic Models with Code Implementation. Method Fig. 1. The forward diffusion process $q$ (left to right) gradually adds Gaussian noise to the target image. The reverse inference process $p$ (right to left) iteratively denoises the target image conditioned on a source image x. (Image source: Saharia et al. 2023) SR3 is a model obtained by improving on DDPM. Instead of randomly generating images, low resolution images are used as conditions to generate images. The main changes in SR3 are:\nThe low resolution image is concatenated to the original input (x_t-1) after bicubic interpolation to get a 6-channel tensor as the new input to the DDPM. We experimented with more sophisticated methods of conditioning, such as using FiLM (Perez et al. 2018), but we found that the simple concatenation yielded similar generation quality.\nInstead of sampling $\\bar{\\alpha}_t$ directly using timestep $t$ to compute the correlation variable and loss, a random value is sampled from the distribution $\\bar{\\alpha} \\sim p(\\bar{\\alpha}) = U(\\bar{\\alpha}_{t-1}, \\bar{\\alpha}_{t})$. (Section 2.4 in Saharia et al. 2023) The model receives noise level $\\bar{\\alpha}_t$ directly instead of timestamp $t$. This allows flexibility in adjusting the noise level and the number of sampling steps during inferring. Experrimental Study New metric: Consistency As a measure of the consistentcy of the superresolution outputs, we compute MSE between the downsampled outputs and the low resolution inputs.\nNew metric: Classification Accuracy In the field of low-level vision, metrics often do not comprehensively represent the quality of images. Therefore the effectiveness of low-level models is often evaluated in terms of proxy tasks.\nThis paper mirror the evalution setup of Zhang et al. (2018) and apply 4$\\times$ superresolution models to 56$\\times$56 center crops from the validation set of ImageNet.\nQuantitative Results Compared to PULSE (Menon et al. 2020), FSRGAN (Chen et al. 2018), and Regressive models, the results in terms of PSNR and SSIM are relatively average. This is because traditional super-resolution models are typically trained based on PSNR, which SR3 is not. Therefore, it is normal for the metrics to be relatively low. However, the consistency metrics, on the other hand, perform very well.\nTable 1. PSNR \u0026 SSIM on 16$\\times$16 $\\to$ 128$\\times$128 face superresolution. Consistency measures MSE ($\\times10^{−5}$) between the lowresolution inputs and the down-sampled super-resolution outputs. (Table source: Saharia et al. 2023 as a screenshot) Table 2. Performance comparison between SR3 and Regression baseline on natural image super-resolution using standard metrics computed on the ImageNet validation set. (Table source: Saharia et al. 2023 as a screenshot) Evaluation of Proxy Task Object recognition baseline: ResNet-50 (He et al. 2016). Table 3. Comparison of classification accuracy scores for 4$\\times$ natural image super-resolution on the first 1K images from the ImageNet Validation set. (Table source: Saharia et al. 2023 as a screenshot) Human Evaluation (2AFC) This paper use a 2-alternative forced-choice (2AFC) paradigm to measure how well humans can discriminate true images from those generated from a model.\nFig. 2. Face super-resolution human fool rates (higher is better, photo-realistic samples yield a fool rate of 50%). Outputs of 4 models are compared against ground truth. (top) Subjects are shown low-resolution inputs. (bottom) Inputs are not shown. (Image source: Saharia et al. 2023) Fig. 3. ImageNet super-resolution fool rates (higher is better, photo-realistic samples yield a fool rate of 50%). SR3 and Regression outputs are compared against ground truth. (top) Subjects are shown low-resolution inputs. (bottom) Inputs are not shown. (Image source: Saharia et al. 2023) Visualization Fig. 3. Comparison of different methods on the 16$\\times$16 $\\to$ 128$\\times$128 face super-resolution task. Reference image has not been included because of privacy concerns. (Image source: Saharia et al. 2023) Fig. 4. Results of a SR3 model (64$\\times$64 $\\to$ 512$\\times$512), trained on FFHQ, and applied to images outside of the training set, along with enlarged patches to show finer details. (Image source: Saharia et al. 2023) Fig. 4 shows that the image obtained by SR3 has more details (high-frequency information of the image) compared to the regression model.\nSummary SR3 employs a completely novel approach to super-resolution, distinct from previous approaches based on GANs and CNNs. It primarily generates high-resolution images by denoising progressively from low resolution images conditioned on diffusion models. In the experimental section, the PSNR and SSIM metrics show relatively less impressive performance compared to other methods. However, it outperforms the Regression model in terms of FID and IS metrics, which would be more convincing if PULSE and FSRGAN also be evaluated. Personally, I find the consistency metric not very meaningful. Still, its remarkable performance in proxy task compared to the Regression model is worth attention (through there is still a lack of experimental comparisons with PULSE and FSRGAN). The approach of using diffusion models for image super-resolution is effective, and there is potential for further research in the future.\nReference [1] Chitwan Saharia et al. “Image Super-Resolution via Iterative Refinement.” TPAMI 2023.\n[2] Ethan Perez et al. “FiLM: Visual Reasoning with a General Conditioning Layer.” AAAI 2018.\n[3] Yulun Zhang et al. “Image Super-Resolution Using Very Deep Residual Channel Attention Networks.” ECCV 2018.\n[4] Sachit Menon et al. “PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models.” CVPR 2020.\n[5] Yu Chen et al. “FSRNet: End-to-End Learning Face Super-Resolution With Facial Priors.” CVPR 2018.\n[6] Kaiming He et al. “Deep residual learning for image recognition.” CVPR 2016.\n","wordCount":"1037","inLanguage":"en","datePublished":"2023-08-05T00:00:00Z","dateModified":"2023-08-05T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://gavinsun0921.github.io/posts/fast-paper-reading-02/"},"publisher":{"@type":"Organization","name":"Gavin Sun · Spatial Intelligence","logo":{"@type":"ImageObject","url":"https://gavinsun0921.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://gavinsun0921.github.io/ accesskey=h title="Spatial Intelligence (Alt + H)">Spatial Intelligence</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gavinsun0921.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://gavinsun0921.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gavinsun0921.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://gavinsun0921.github.io/about/ title=AboutMe><span>AboutMe</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告</h1><div class=post-meta><span title='2023-08-05 00:00:00 +0000 UTC'>August 5, 2023</span>&nbsp;·&nbsp;<span>3 min</span>&nbsp;·&nbsp;<span>1037 words</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#overview aria-label=Overview>Overview</a><ul><li><a href=#problem aria-label=Problem>Problem</a></li><li><a href=#solution aria-label=Solution>Solution</a></li><li><a href=#results aria-label=Results>Results</a></li></ul></li><li><a href=#related-work aria-label="Related Work">Related Work</a><ul><li><a href=#diffusion-probabilistic-models aria-label="Diffusion Probabilistic Models">Diffusion Probabilistic Models</a></li></ul></li><li><a href=#method aria-label=Method>Method</a></li><li><a href=#experrimental-study aria-label="Experrimental Study">Experrimental Study</a><ul><li><a href=#new-metric-consistency aria-label="New metric: Consistency">New metric: Consistency</a></li><li><a href=#new-metric-classification-accuracy aria-label="New metric: Classification Accuracy">New metric: Classification Accuracy</a></li><li><a href=#quantitative-results aria-label="Quantitative Results">Quantitative Results</a></li><li><a href=#evaluation-of-proxy-task aria-label="Evaluation of Proxy Task">Evaluation of Proxy Task</a></li><li><a href=#human-evaluation-2afc aria-label="Human Evaluation (2AFC)">Human Evaluation (2AFC)</a></li><li><a href=#visualization aria-label=Visualization>Visualization</a></li></ul></li><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><ul><li><strong><a href=https://ieeexplore.ieee.org/document/9887996><strong>This paper</strong></a> is published in TPAMI 2023.</strong></li></ul><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><h3 id=problem>Problem<a hidden class=anchor aria-hidden=true href=#problem>#</a></h3><ul><li>In the field of image super-resolution, existing approaches often suffer from various limitations; e.g., autoregressive models are prohibitively expensive for high-resolution image generation, Normalizing Flows (NFs) and variational autoencoders (VAEs) often yield sub-optimal sample quality, and GANs require carefully designed regularization and optimization tricks to tame optimization instability and model collapse.</li></ul><h3 id=solution>Solution<a hidden class=anchor aria-hidden=true href=#solution>#</a></h3><ul><li>Present <strong>SR3</strong>, an approach to image super-resolution via repeated refinement based on DDPM.</li></ul><h3 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h3><ul><li>The high-frequency information of the image can be well resored compared to other methods.</li><li>Despite mediocre performance in SSIM and PSNR metrics, visualization and consistency are good.</li></ul><h2 id=related-work>Related Work<a hidden class=anchor aria-hidden=true href=#related-work>#</a></h2><h3 id=diffusion-probabilistic-models>Diffusion Probabilistic Models<a hidden class=anchor aria-hidden=true href=#diffusion-probabilistic-models>#</a></h3><p>I&rsquo;ve written a blog about diffusion probabilistic models (DPM). It has the derivation of the basic formulas of the DPM as well as a simple code implementation.</p><ul><li><a href=https://gavinsun0921.github.io/posts/paper-reading-01/>A Breif Exploration to Diffusion Probabilistic Models with Code Implementation</a>.</li></ul><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><div align=center><img src=conditional.png style=zoom:40% alt>
Fig. 1. The forward diffusion process $q$ (left to right) gradually adds<br>Gaussian noise to the target image. The reverse inference process<br>$p$ (right to left) iteratively denoises the target image conditioned<br>on a source image x. (Image source: <a href=https://ieeexplore.ieee.org/document/9887996>Saharia <i>et al.</i> 2023</a>)</div><p>SR3 is a model obtained by improving on DDPM. Instead of randomly generating images, low resolution images are used as conditions to generate images. The main changes in SR3 are:</p><ol><li>The low resolution image is concatenated to the original input (x_t-1) after bicubic interpolation to get a 6-channel tensor as the new input to the DDPM.<blockquote><p>We experimented with more sophisticated methods of conditioning, such as using FiLM (<a href=https://ojs.aaai.org/index.php/AAAI/article/view/11671>Perez <em>et al.</em> 2018</a>), but we found that the simple concatenation yielded similar generation quality.</p></blockquote></li><li>Instead of sampling $\bar{\alpha}_t$ directly using timestep $t$ to compute the correlation variable and loss, a random value is sampled from the distribution $\bar{\alpha} \sim p(\bar{\alpha}) = U(\bar{\alpha}_{t-1}, \bar{\alpha}_{t})$. (Section 2.4 in <a href=https://ieeexplore.ieee.org/document/9887996>Saharia <em>et al.</em> 2023</a>)</li><li>The model receives noise level $\bar{\alpha}_t$ directly instead of timestamp $t$. This allows flexibility in adjusting the noise level and the number of sampling steps during inferring.</li></ol><h2 id=experrimental-study>Experrimental Study<a hidden class=anchor aria-hidden=true href=#experrimental-study>#</a></h2><h3 id=new-metric-consistency>New metric: Consistency<a hidden class=anchor aria-hidden=true href=#new-metric-consistency>#</a></h3><blockquote><p>As a measure of the consistentcy of the superresolution outputs, we compute MSE between the downsampled outputs and the low resolution inputs.</p></blockquote><h3 id=new-metric-classification-accuracy>New metric: Classification Accuracy<a hidden class=anchor aria-hidden=true href=#new-metric-classification-accuracy>#</a></h3><p>In the field of low-level vision, metrics often do not comprehensively represent the quality of images. Therefore the effectiveness of low-level models is often evaluated in terms of proxy tasks.</p><p>This paper mirror the evalution setup of <a href=https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html>Zhang <em>et al.</em> (2018)</a> and apply 4$\times$ superresolution models to 56$\times$56 center crops from the validation set of ImageNet.</p><h3 id=quantitative-results>Quantitative Results<a hidden class=anchor aria-hidden=true href=#quantitative-results>#</a></h3><p>Compared to PULSE (<a href=https://openaccess.thecvf.com/content_CVPR_2020/html/Menon_PULSE_Self-Supervised_Photo_Upsampling_via_Latent_Space_Exploration_of_Generative_CVPR_2020_paper.html>Menon <em>et al.</em> 2020</a>), FSRGAN (<a href=https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html>Chen <em>et al.</em> 2018</a>), and Regressive models, the results in terms of PSNR and SSIM are relatively average. This is because traditional super-resolution models are typically trained based on PSNR, which SR3 is not. Therefore, it is normal for the metrics to be relatively low. However, the consistency metrics, on the other hand, perform very well.</p><div align=center>Table 1. PSNR & SSIM on 16$\times$16 $\to$ 128$\times$128 face superresolution.<br>Consistency measures MSE ($\times10^{−5}$) between the lowresolution<br>inputs and the down-sampled super-resolution outputs. (Table<br>source: <a href=https://ieeexplore.ieee.org/document/9887996>Saharia <i>et al.</i> 2023</a> as a screenshot)
<img src=table1.png style=zoom:30% alt></div><div align=center>Table 2. Performance comparison between SR3 and Regression<br>baseline on natural image super-resolution using standard<br>metrics computed on the ImageNet validation set. (Table<br>source: <a href=https://ieeexplore.ieee.org/document/9887996>Saharia <i>et al.</i> 2023</a> as a screenshot)
<img src=table2.png style=zoom:33% alt></div><h3 id=evaluation-of-proxy-task>Evaluation of Proxy Task<a hidden class=anchor aria-hidden=true href=#evaluation-of-proxy-task>#</a></h3><ul><li>Object recognition baseline: ResNet-50 (<a href=https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html>He <em>et al.</em> 2016</a>).</li></ul><div align=center>Table 3. Comparison of classification accuracy scores for 4$\times$ natural<br>image super-resolution on the first 1K images from the ImageNet<br>Validation set. (Table source: <a href=https://ieeexplore.ieee.org/document/9887996>Saharia <i>et al.</i> 2023</a> as a screenshot)
<img src=table3.png style=zoom:35% alt></div><h3 id=human-evaluation-2afc>Human Evaluation (2AFC)<a hidden class=anchor aria-hidden=true href=#human-evaluation-2afc>#</a></h3><p>This paper use a 2-alternative forced-choice (2AFC) paradigm to measure how well humans can discriminate true images from those generated from a model.</p><div align=center><img src=human1.png style=zoom:45% alt>
Fig. 2. Face super-resolution human fool rates (higher is better,<br>photo-realistic samples yield a fool rate of 50%). Outputs of 4<br>models are compared against ground truth. (top) Subjects are<br>shown low-resolution inputs. (bottom) Inputs are not shown.<br>(Image source: <a href=https://ieeexplore.ieee.org/document/9887996>Saharia <i>et al.</i> 2023</a>)</div><div align=center><img src=human2.png style=zoom:45% alt>
Fig. 3. ImageNet super-resolution fool rates (higher is better,<br>photo-realistic samples yield a fool rate of 50%). SR3 and Regression<br>outputs are compared against ground truth. (top) Subjects are shown<br>low-resolution inputs. (bottom) Inputs are not shown. (Image source:<br><a href=https://ieeexplore.ieee.org/document/9887996>Saharia <i>et al.</i> 2023</a>)</div><h3 id=visualization>Visualization<a hidden class=anchor aria-hidden=true href=#visualization>#</a></h3><div align=center><img src=vision1.png style=zoom:45% alt>
Fig. 3. Comparison of different methods on the 16$\times$16 $\to$ 128$\times$128 face<br>super-resolution task. Reference image has not been included because<br>of privacy concerns. (Image source: <a href=https://ieeexplore.ieee.org/document/9887996>Saharia <i>et al.</i> 2023</a>)</div><div align=center><img src=vision2.png style=zoom:75% alt>
Fig. 4. Results of a SR3 model (64$\times$64 $\to$ 512$\times$512), trained on FFHQ, and applied to<br>images outside of the training set, along with enlarged patches to show finer details.<br>(Image source: <a href=https://ieeexplore.ieee.org/document/9887996>Saharia <i>et al.</i> 2023</a>)</div><p>Fig. 4 shows that the image obtained by SR3 has more details (high-frequency information of the image) compared to the regression model.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>SR3 employs a completely novel approach to super-resolution, distinct from previous approaches based on GANs and CNNs. It primarily generates high-resolution images by denoising progressively from low resolution images conditioned on diffusion models. In the experimental section, the PSNR and SSIM metrics show relatively less impressive performance compared to other methods. However, it outperforms the Regression model in terms of FID and IS metrics, which would be more convincing if PULSE and FSRGAN also be evaluated. Personally, I find the consistency metric not very meaningful. Still, its remarkable performance in proxy task compared to the Regression model is worth attention (through there is still a lack of experimental comparisons with PULSE and FSRGAN). The approach of using diffusion models for image super-resolution is effective, and there is potential for further research in the future.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p>[1] Chitwan Saharia et al. <a href=https://ieeexplore.ieee.org/document/9887996>&ldquo;Image Super-Resolution via Iterative Refinement.&rdquo;</a> TPAMI 2023.</p><p>[2] Ethan Perez et al. <a href=https://ojs.aaai.org/index.php/AAAI/article/view/11671>&ldquo;FiLM: Visual Reasoning with a General Conditioning Layer.&rdquo;</a> AAAI 2018.</p><p>[3] Yulun Zhang et al. <a href=https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html>&ldquo;Image Super-Resolution Using Very Deep Residual Channel Attention Networks.&rdquo;</a> ECCV 2018.</p><p>[4] Sachit Menon et al. <a href=https://openaccess.thecvf.com/content_CVPR_2020/html/Menon_PULSE_Self-Supervised_Photo_Upsampling_via_Latent_Space_Exploration_of_Generative_CVPR_2020_paper.html>&ldquo;PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models.&rdquo;</a> CVPR 2020.</p><p>[5] Yu Chen et al. <a href=https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html>&ldquo;FSRNet: End-to-End Learning Face Super-Resolution With Facial Priors.&rdquo;</a> CVPR 2018.</p><p>[6] Kaiming He et al. <a href=https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html>&ldquo;Deep residual learning for image recognition.&rdquo;</a> CVPR 2016.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://gavinsun0921.github.io/tags/diffusion/>Diffusion</a></li><li><a href=https://gavinsun0921.github.io/tags/dpm/>DPM</a></li><li><a href=https://gavinsun0921.github.io/tags/computer-vision/>Computer Vision</a></li><li><a href=https://gavinsun0921.github.io/tags/low-level-vision/>Low-Level Vision</a></li><li><a href=https://gavinsun0921.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://gavinsun0921.github.io/posts/fast-paper-reading-03/><span class=title>« Prev</span><br><span>[NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 阅读报告</span>
</a><a class=next href=https://gavinsun0921.github.io/posts/fast-paper-reading-01/><span class=title>Next »</span><br><span>[CVPR'22] Deblurring via Stochastic Refinement 阅读报告</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告 on x" href="https://x.com/intent/tweet/?text=%5bT-PAMI%2723%5d%20Image%20Super-Resolution%20via%20Iterative%20Refinement%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-02%2f&amp;hashtags=Diffusion%2cDPM%2cComputerVision%2cLow-levelVision%2cDeepLearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-02%2f&amp;title=%5bT-PAMI%2723%5d%20Image%20Super-Resolution%20via%20Iterative%20Refinement%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;summary=%5bT-PAMI%2723%5d%20Image%20Super-Resolution%20via%20Iterative%20Refinement%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;source=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-02%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-02%2f&title=%5bT-PAMI%2723%5d%20Image%20Super-Resolution%20via%20Iterative%20Refinement%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-02%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告 on whatsapp" href="https://api.whatsapp.com/send?text=%5bT-PAMI%2723%5d%20Image%20Super-Resolution%20via%20Iterative%20Refinement%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a%20-%20https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-02%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告 on telegram" href="https://telegram.me/share/url?text=%5bT-PAMI%2723%5d%20Image%20Super-Resolution%20via%20Iterative%20Refinement%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-02%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [T-PAMI'23] Image Super-Resolution via Iterative Refinement 阅读报告 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bT-PAMI%2723%5d%20Image%20Super-Resolution%20via%20Iterative%20Refinement%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-02%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=GavinSun0921/gavinsun0921.github.io data-repo-id=R_kgDOJgiWSg data-category=Announcements data-category-id=DIC_kwDOJgiWSs4CtrHs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://gavinsun0921.github.io/>Gavin Sun · Spatial Intelligence</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>