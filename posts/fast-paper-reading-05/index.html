<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告 | Gavin Sun · Spatial Intelligence</title><meta name=keywords content="Point Tracking,4D Reconstruction,Computer Vision"><meta name=description content="本文巧妙地提出了一种“运动解耦”机制，通过一个学习的 3D Tracker 将动态物体的自身运动从观测运动中剥离，使得经典的 Bundle Adjustment 能够首次被统一地应用于含动态物体的场景中，极大地提升了动态场景重建中的相机位姿精度和三维重建质量。"><meta name=author content><link rel=canonical href=https://gavinsun0921.github.io/posts/fast-paper-reading-05/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://gavinsun0921.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gavinsun0921.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gavinsun0921.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gavinsun0921.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gavinsun0921.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gavinsun0921.github.io/posts/fast-paper-reading-05/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredTags:["script","noscript","style","textarea","pre"],throwOnError:!1})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-18P7N9RZDS"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-18P7N9RZDS")</script><meta property="og:url" content="https://gavinsun0921.github.io/posts/fast-paper-reading-05/"><meta property="og:site_name" content="Gavin Sun · Spatial Intelligence"><meta property="og:title" content="[ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告"><meta property="og:description" content="本文巧妙地提出了一种“运动解耦”机制，通过一个学习的 3D Tracker 将动态物体的自身运动从观测运动中剥离，使得经典的 Bundle Adjustment 能够首次被统一地应用于含动态物体的场景中，极大地提升了动态场景重建中的相机位姿精度和三维重建质量。"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-04T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-04T00:00:00+00:00"><meta property="article:tag" content="Point Tracking"><meta property="article:tag" content="4D Reconstruction"><meta property="article:tag" content="Computer Vision"><meta name=twitter:card content="summary"><meta name=twitter:title content="[ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告"><meta name=twitter:description content="本文巧妙地提出了一种“运动解耦”机制，通过一个学习的 3D Tracker 将动态物体的自身运动从观测运动中剥离，使得经典的 Bundle Adjustment 能够首次被统一地应用于含动态物体的场景中，极大地提升了动态场景重建中的相机位姿精度和三维重建质量。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gavinsun0921.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告","item":"https://gavinsun0921.github.io/posts/fast-paper-reading-05/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告","name":"[ICCV\u002725 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告","description":"本文巧妙地提出了一种“运动解耦”机制，通过一个学习的 3D Tracker 将动态物体的自身运动从观测运动中剥离，使得经典的 Bundle Adjustment 能够首次被统一地应用于含动态物体的场景中，极大地提升了动态场景重建中的相机位姿精度和三维重建质量。","keywords":["Point Tracking","4D Reconstruction","Computer Vision"],"articleBody":"TL;DR 本文巧妙地提出了一种“运动解耦”机制，通过一个学习的 3D Tracker 将动态物体的自身运动从观测运动中剥离，使得经典的 Bundle Adjustment 能够首次被统一地应用于含动态物体的场景中，极大地提升了动态场景重建中的相机位姿精度和三维重建质量。\n1 研究动机 (Motivation) 1.1 背景 (Background) 从 casual video 中进行动态场景重建，对于 AR、机器人等应用至关重要。传统的 SLAM 和 SfM 方法依赖于经典的 Bundle Adjustment (BA)，在静态场景中表现优异，能够实现高精度的相机位姿和场景几何恢复。\n1.2 现有方法的局限性 (Gap/Limitations of Existing Work) 做着指出现有方法的“无人区”：\n经典方法的失效：传统 BA 和 SLAM 系统强依赖于静态世界假设（即 epipolar constraint），而动态物体完全破坏了这个假设，导致这些方法在动态场景中会失效或产生严重错误。 ”删除“策略的缺陷：一些方法通过检测并过滤掉动态区域来保证 BA 的运行。但这会严重导致重建结果不完整，丢失了所有动态物体的几何信息，这在很多应用场景中是不可接受的。 “独立建模”策略的困难：另一些方法尝试为动态物体建立独立的运动模型，但这通常很复杂，并且容易导致运动估计不一致的问题。 “深度先验”策略的瓶颈：最近的工作利用了单目深度估计作为先验，但这些深度图在时序上往往存在不一致（inconsistent），尤其是尺度（scale）不一致，导致将它们融合成全局一致的三维模型非常困难。 1.3 本文价值 (Value Proposition) 这篇论文的价值主张极具颠覆性——我们不必再“绕着走”了。作者没有选择删除动态点或为其建立复杂的模型，而是提出了一个全新的思路：我们是否可以将动态点的运动“中和”掉，让 BA 认为它们也是“静态”的？这使得强大而成熟的 BA 优化框架得以重返（Back on Track）动态场景重建的核心舞台。\n2. 解决的关键问题与贡献 (Key Problem Solved \u0026 Contribution) 2.1 解决的关键技术问题 如何改造输入信息，使得经典且强大的 Bundle Adjustment 优化框架能够直接、统一地处理包含大量动态物体的场景，从而在不丢弃动态信息的前提下，同时实现高精度的相机位姿和时序一致的密集场景重建？\n2.2 核心贡献 核心思想：运动解耦（Motion Decoupling）：这是本文的灵魂。作者提出，任何一个点的观测运动（total motion）都可以被分解为相机运动引发的静态分量（static component）和物体自身运动引发的动态分量（dynamic component）。通过只把这个“静态分量”喂给 BA，BA 就可以像处理静态点一样处理动态点，从而统一了整个优化过程。 双追踪器架构（Dual-Tracker Architecture）：为了准确地实现运动解耦，做着设计了一个由两个 Transformer 追踪器组成的 Front-end。一个追踪器 $T$ 负责预测总运动 $\\mathbf{X}_{\\text{total}}$，另一个更小更高效的追踪器 ${T}_{\\text{dyn}}$ 专门预测动态分量 $\\mathbf{X}_{\\text{dyn}}$。这个设计在实验中被证明优于单一追踪器直接预测静态分量的方案。 统一框架 BA-Track：提出了一个由三个阶段组成的完整 pipeline： Stage 1: 基于学习的运动解耦 3D 追踪器，分离出运动的静态分量。 Stage 2: 经典的 Bundle Adjustment，利用静态运动分量稳健地估计相机位姿和稀疏场景结构。 Stage 3: 轻量级的全局优化，利用 BA 得到的稀疏精确点来优化初始的密集深度图，得到全局一致的密集重建。 3. 方法详述 (Method) Fig. 1. BA-Track Framework\n3.1 Stage I: 运动解耦的3D追踪器 (Motion-Decoupled 3D Tracker) 这一阶段是整个框架的基石。它的核心目标不是简单地追踪点，而是要从任何一个点的总观测运动中，精准地分离出纯粹由相机运动引起的静态分量（static component）。\n这个思想在图 2 中非常直观的展示了，对于一个动态物体上的点，我们观测到的总运动 $\\mathbf{X}_{\\text{total}}$（蓝色向量），实际上是相机自身运动 $\\mathbf{X}_{\\text{static}}$（橙色向量）和物体自身运动 $\\mathbf{X}_{\\text{dyn}}$（紫色向量）的叠加。\nFig. 2. Illustration of motion decoupling\n为了实现这个解耦，作者设计了一个巧妙的 Front-end：\n双追踪器架构 (Dual-Tracker Architecture): 作者没有用一个网络去硬解这个难题，而是用了两个基于 Transformer 的追踪器。 主追踪器 $T$: 负责预测点的总观测运动 $\\mathbf{X}_{\\text{total}}$，同时还预测该点的可见性 $v$ 和一个动态标签 $m$（一个0到1之间的软标签，表示该点属于动态物体的概率）。 动态追踪器 $T_{\\text{dyn}}$: 这是一个更小、更高效的网络（层数是 $T$ 的一半），专门负责预测物体自身的动态运动分量 $\\mathbf{X}_{\\text{dyn}}$。 为什么用两个？论文在消融实验中证明，让一个网络同时学习追踪和理解复杂的运动模式是次优的。通过将任务分解给两个网络，每个网络可以更专注，从而实现更准确的解耦。 运动解耦公式: $$ \\mathbf{X}_{\\text{static}} = \\mathbf{X}_{\\text{total}} - m \\cdot \\mathbf{X}_{\\text{dyn}} \\tag{1} $$ 如果一个点是静态的（$m$ 趋近于0），那么 $\\mathbf{X}_{\\text{static}}$ 就约等于 $\\mathbf{X}_{\\text{total}}$，这符合事实。 如果一个点是动态的（$m$ 趋近于1），那么它的静态分量 $\\mathbf{X}_{\\text{static}}$ 就是其总运动减去物体自身运动。这样一来，即使是动态物体上的点，其 $\\mathbf{X}_{\\text{static}}$ 也只反映了相机的运动，使得它在 BA 看来和一个静态点是等价的。 3.2 Stage II: Bundle Adjustment (BA) 利用第一阶段“净化”后的运动信息 $\\mathbf{X}_{\\text{static}}$，运行一个鲁棒的 BA，以高精度地恢复相机位姿和场景的稀疏三维结构。\nBA 是一个经典的优化问题，通过最小化重投影误差来同时优化相机参数和三维点坐标。本作的关键在于，它将 BA 的输入进行了“改造”。\n“BA友好”的输入: BA 处理的不再是原始的、带有动态噪声的观测点，而是 Front-end 输出的运动静态分量 $\\mathbf{X}_{\\text{static}}$。这使得经典的BA框架几乎无需修改就可以直接应用于动态场景，因为动态物体的“干扰”已经在输入端被消除了。 优化目标: $$\\argmin_{{\\mathbf{T}_t},{\\mathbf{Y}}} \\sum_{|i-j|\\leq S} \\sum_n W^i_n(j) | \\mathcal{P}_j(\\mathbf{x}^i_n, y^i_n) - X^t_n(j) |_\\rho + \\alpha | y^i_n - d(\\mathbf{X}^i_n) |^2 \\tag{2}$$ 优化变量: 所有帧的相机位姿 $\\{\\mathbf{T}_t\\}$ 和所有稀疏查询点的精确深度 $\\{\\mathbf{Y}\\}$。 第一项：重投影误差: $\\mathcal{P}_j$ 是标准的投影函数，它将第 $i$ 帧的点 $\\mathbf{x}^i_n$ 根据当前的相机位姿估计 $\\{\\mathbf{T}_t\\}$ 投影到第 $j$ 帧。误差是这个投影点与 Front-end 输出的追踪点 $X^i_n(j)$ 之间的距离。 第二项：深度一致性: 这是一个正则项，它鼓励优化后的深度 $y_n^i$ 不要偏离初始的深度先验 $d(\\mathbf{X}^i_n)$ 太远，由超参数 $\\alpha$ 控制权重。 权重 $W$: 权重 $W_n^i(j)$ 包含了可见性 $v$ 和动态标签 $m$ 的信息，形式为 $v \\cdot (1-m)$。这意味着，在优化位姿时，BA 会更相信那些可见且被判断为静态的点，从而进一步提升了相机位姿估计的鲁棒性。 3.3 Stage III: 全局优化 (Global Refinement) BA 只输出了精确的相机位姿和稀疏的三维点。而初始的深度图是密集但可能不准确且时序不一致的。此阶段的目标就是利用 BA 输出的精确稀疏点，来对齐和优化整个视频的密集深度图，得到全局一致的密集重建结果。\n作者没有直接对密集的深度图进行复杂变形，而是采用了一种更高效的 scale map 方法。\n可变形的 scale map: 对每一帧，模型学习一个比原图分辨率低的 2D scale map $\\theta_t$ 深度优化公式: 最终的优化后深度由一个简单的乘法得到 $$ \\hat{D}_t[\\mathbf{x}] = \\theta_t[\\mathbf{x}]\\cdot D_t[\\mathbf{x}] $$ 其中 $D_t[\\mathbf{x}]$ 是初始深度，$\\theta_t[\\mathbf{x}]$ 是从低分辨率 scale map 中通过双线性插值得到的缩放因子。这意味着整个优化过程是学习一个平滑的、空间可变的缩放场来校正初始深度。 两个核心损失函数: $\\mathcal{L}_\\text{depth}$: 深度一致性损失。它强制要求，在稀疏点的那些位置上，优化后的密集深度 $\\hat{D}_t$ 应该要等于 BA 阶段得到的精确稀疏深度。这相当于用精确的稀疏点作为“锚点”来校准整个密集深度场。 $\\mathcal{L}_\\text{rigid}$: 场景刚性损失，它要求场景中静态部分的任意两点之间的三维距离，在不同帧之间应该保持不变。这个损失通过一个权重 $W_\\text{static}$ 来确保只对静态点生效，从而防止了静态背景在优化过程中发生不应有的形变。 通过这三个阶段的紧密协作，BA-Track 成功地将经典 BA 的强大优化能力引入到充满挑战的动态世界中，实现了非常出色的效果。\n4. 实验分析 (Experiments) 4.1 Camera Pose Estimation Tab. 1. Camera pose evaluation results on Sintel, Shibuya, and Epic Fields datasets\nFig. 3. Qualitative camera pose estimation results on Sintel, Shibuya, and Epic Fields datasets\n如图 3 所示，直观展示了 BA-Track 相机轨迹相比其他方法更平滑、更接近 ground truth，而其他方法在这些场景下往往表现不佳。在具有快速动态内容和复杂相机运动的 Epic Fields 数据集上，BA-Track 仍能保持出色的 VO accuracy。\n4.2 Depth Evaluation Tab. 2. Depth evaluation results on Sintel, Shibuya, and Bonn datasets\n基于尺度地图的优化方法在参数较少的情况下实现了高效的优化，同时保持了相对简单的结构。\n4.3 Motion Decoupling Fig. 4. Motion decoupling on the DAVIS dataset\n图 4 展示了在 DAVIS 数据集上观测到的点运动与静态点运动情况。红色点表示动态轨迹，绿色点代表静态点，突显了三维追踪器分离观测运动中动态成分的能力。\n4.4 Ablation Study 4.4.1 Dual-Tracker Architecture Tab. 3. Ablation study of dynamic handling methods on Sintel\n4.4.2 Global Refinement Tab. 4. Ablation study on depth refinement on Bonn crowd2 sequence\n分别尝试关闭 $\\mathcal{L}_\\text{depth}$ 和 $\\mathcal{L}_\\text{rigid}$ 进行联合关闭与交替关闭的实验。完全移除两项损失函数会导致重建精度下降。\nFig. 5. Visualization of global refinement on the DAVIS\n图 5 展示了采用与不采用我们深度优化方法进行重建的两种直观对比。直接将单目深度图与估计的相机位姿融合，会导致重建结果不一致，并出现重复的三维结构，而我们的全局优化显著提升了三维一致性。\n5. 批判性思考 (Critical Analysis \u0026 Personal Thoughts) 5.1 优点 (Strengths) 思想极其优雅 (Conceptual Elegance): 本文的核心 idea 非常漂亮。它没有去硬碰硬地为动态物体建模，而是通过“运动解耦”四两拨千斤，让经典的 BA 工具重获新生。这种解决问题的思路本身就极具启发性，是顶会 Oral 的典范。 效果非常扎实: 不仅提出了新颖的理论，更在多个高难度动态场景基准上取得了 SOTA 的性能，用实验结果充分证明了想法的有效性。 完美的混合系统: 它是“传统几何优化”与“深度学习感知”成功结合的又一力作。用学习的追踪器来做它擅长的事（提供鲁棒的对应和运动先验），用经典的 BA 来做它擅长的事（高精度、有几何约束的优化），强强联合。 5.2 潜在缺点/疑问 (Weaknesses/Questions) 对深度先验的依赖: 整个 pipeline 的起点是第三方的单目深度估计网络。如果初始深度质量很差（例如在透明、反光物体上），可能会影响第一阶段运动解耦的准确性，进而误差会传导至后续的 BA 和全局优化。 运动解耦的鲁棒性: 核心公式 $\\mathbf{X}_\\text{static} = \\mathbf{X}_\\text{total} - m \\cdot \\mathbf{X}_\\text{dyn}$ 的成败，高度依赖于动态标签 $m$ 和动态运动 $\\mathbf{X}_\\text{dyn}$ 的预测精度。当动态和静态界限模糊，或物体运动模式罕见时，Front-end 的预测错误可能会对最终结果产生较大影响。 对相机内参的假设: 论文中假设相机内参是已知的 。虽然作者在附录中讨论了未来可以联合优化内参，但这在当前版本中仍是一个限制。 ","wordCount":"4114","inLanguage":"en","datePublished":"2025-08-04T00:00:00Z","dateModified":"2025-08-04T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://gavinsun0921.github.io/posts/fast-paper-reading-05/"},"publisher":{"@type":"Organization","name":"Gavin Sun · Spatial Intelligence","logo":{"@type":"ImageObject","url":"https://gavinsun0921.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://gavinsun0921.github.io/ accesskey=h title="Spatial Intelligence (Alt + H)">Spatial Intelligence</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gavinsun0921.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://gavinsun0921.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gavinsun0921.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://gavinsun0921.github.io/about/ title=AboutMe><span>AboutMe</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告</h1><div class=post-meta><span title='2025-08-04 00:00:00 +0000 UTC'>August 4, 2025</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>4114 words</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tldr aria-label=TL;DR>TL;DR</a></li><li><a href=#1-%e7%a0%94%e7%a9%b6%e5%8a%a8%e6%9c%ba-motivation aria-label="1 研究动机 (Motivation)">1 研究动机 (Motivation)</a><ul><li><a href=#11-%e8%83%8c%e6%99%af-background aria-label="1.1 背景 (Background)">1.1 背景 (Background)</a></li><li><a href=#12-%e7%8e%b0%e6%9c%89%e6%96%b9%e6%b3%95%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7-gaplimitations-of-existing-work aria-label="1.2 现有方法的局限性 (Gap/Limitations of Existing Work)">1.2 现有方法的局限性 (Gap/Limitations of Existing Work)</a></li><li><a href=#13-%e6%9c%ac%e6%96%87%e4%bb%b7%e5%80%bc-value-proposition aria-label="1.3 本文价值 (Value Proposition)">1.3 本文价值 (Value Proposition)</a></li></ul></li><li><a href=#2-%e8%a7%a3%e5%86%b3%e7%9a%84%e5%85%b3%e9%94%ae%e9%97%ae%e9%a2%98%e4%b8%8e%e8%b4%a1%e7%8c%ae-key-problem-solved--contribution aria-label="2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)">2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)</a><ul><li><a href=#21-%e8%a7%a3%e5%86%b3%e7%9a%84%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e9%97%ae%e9%a2%98 aria-label="2.1 解决的关键技术问题">2.1 解决的关键技术问题</a></li><li><a href=#22-%e6%a0%b8%e5%bf%83%e8%b4%a1%e7%8c%ae aria-label="2.2 核心贡献">2.2 核心贡献</a></li></ul></li><li><a href=#3-%e6%96%b9%e6%b3%95%e8%af%a6%e8%bf%b0-method aria-label="3. 方法详述 (Method)">3. 方法详述 (Method)</a><ul><li><a href=#31-stage-i-%e8%bf%90%e5%8a%a8%e8%a7%a3%e8%80%a6%e7%9a%843d%e8%bf%bd%e8%b8%aa%e5%99%a8-motion-decoupled-3d-tracker aria-label="3.1 Stage I: 运动解耦的3D追踪器 (Motion-Decoupled 3D Tracker)">3.1 Stage I: 运动解耦的3D追踪器 (Motion-Decoupled 3D Tracker)</a></li><li><a href=#32-stage-ii-bundle-adjustment-ba aria-label="3.2 Stage II: Bundle Adjustment (BA)">3.2 Stage II: Bundle Adjustment (BA)</a></li><li><a href=#33-stage-iii-%e5%85%a8%e5%b1%80%e4%bc%98%e5%8c%96-global-refinement aria-label="3.3 Stage III: 全局优化 (Global Refinement)">3.3 Stage III: 全局优化 (Global Refinement)</a></li></ul></li><li><a href=#4-%e5%ae%9e%e9%aa%8c%e5%88%86%e6%9e%90-experiments aria-label="4. 实验分析 (Experiments)">4. 实验分析 (Experiments)</a><ul><li><a href=#41-camera-pose-estimation aria-label="4.1 Camera Pose Estimation">4.1 Camera Pose Estimation</a></li><li><a href=#42-depth-evaluation aria-label="4.2 Depth Evaluation">4.2 Depth Evaluation</a></li><li><a href=#43-motion-decoupling aria-label="4.3 Motion Decoupling">4.3 Motion Decoupling</a></li><li><a href=#44-ablation-study aria-label="4.4 Ablation Study">4.4 Ablation Study</a><ul><li><a href=#441-dual-tracker-architecture aria-label="4.4.1 Dual-Tracker Architecture">4.4.1 Dual-Tracker Architecture</a></li><li><a href=#442-global-refinement aria-label="4.4.2 Global Refinement">4.4.2 Global Refinement</a></li></ul></li></ul></li><li><a href=#5-%e6%89%b9%e5%88%a4%e6%80%a7%e6%80%9d%e8%80%83-critical-analysis--personal-thoughts aria-label="5. 批判性思考 (Critical Analysis & Personal Thoughts)">5. 批判性思考 (Critical Analysis & Personal Thoughts)</a><ul><li><a href=#51-%e4%bc%98%e7%82%b9-strengths aria-label="5.1 优点 (Strengths)">5.1 优点 (Strengths)</a></li><li><a href=#52-%e6%bd%9c%e5%9c%a8%e7%bc%ba%e7%82%b9%e7%96%91%e9%97%ae-weaknessesquestions aria-label="5.2 潜在缺点/疑问 (Weaknesses/Questions)">5.2 潜在缺点/疑问 (Weaknesses/Questions)</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=tldr>TL;DR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>本文巧妙地提出了一种“运动解耦”机制，通过一个学习的 3D Tracker 将动态物体的自身运动从观测运动中剥离，使得经典的 Bundle Adjustment 能够首次被统一地应用于含动态物体的场景中，极大地提升了动态场景重建中的相机位姿精度和三维重建质量。</p><h2 id=1-研究动机-motivation>1 研究动机 (Motivation)<a hidden class=anchor aria-hidden=true href=#1-研究动机-motivation>#</a></h2><h3 id=11-背景-background>1.1 背景 (Background)<a hidden class=anchor aria-hidden=true href=#11-背景-background>#</a></h3><p>从 casual video 中进行动态场景重建，对于 AR、机器人等应用至关重要。传统的 SLAM 和 SfM 方法依赖于经典的 <strong>Bundle Adjustment (BA)</strong>，在静态场景中表现优异，能够实现高精度的相机位姿和场景几何恢复。</p><h3 id=12-现有方法的局限性-gaplimitations-of-existing-work>1.2 现有方法的局限性 (Gap/Limitations of Existing Work)<a hidden class=anchor aria-hidden=true href=#12-现有方法的局限性-gaplimitations-of-existing-work>#</a></h3><p>做着指出现有方法的“无人区”：</p><ol><li><strong>经典方法的失效</strong>：传统 BA 和 SLAM 系统强依赖于静态世界假设（即 epipolar constraint），而动态物体完全破坏了这个假设，导致这些方法在动态场景中会失效或产生严重错误。</li><li><strong>”删除“策略的缺陷</strong>：一些方法通过检测并过滤掉动态区域来保证 BA 的运行。但这会严重导致重建结果不完整，丢失了所有动态物体的几何信息，这在很多应用场景中是不可接受的。</li><li><strong>“独立建模”策略的困难</strong>：另一些方法尝试为动态物体建立独立的运动模型，但这通常很复杂，并且容易导致运动估计不一致的问题。</li><li><strong>“深度先验”策略的瓶颈</strong>：最近的工作利用了单目深度估计作为先验，但这些深度图在时序上往往存在不一致（inconsistent），尤其是尺度（scale）不一致，导致将它们融合成全局一致的三维模型非常困难。</li></ol><h3 id=13-本文价值-value-proposition>1.3 本文价值 (Value Proposition)<a hidden class=anchor aria-hidden=true href=#13-本文价值-value-proposition>#</a></h3><p>这篇论文的价值主张极具颠覆性——<strong>我们不必再“绕着走”了</strong>。作者没有选择删除动态点或为其建立复杂的模型，而是提出了一个全新的思路：我们是否可以将动态点的运动“中和”掉，让 BA 认为它们也是“静态”的？这使得强大而成熟的 BA 优化框架得以重返（Back on Track）动态场景重建的核心舞台。</p><h2 id=2-解决的关键问题与贡献-key-problem-solved--contribution>2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)<a hidden class=anchor aria-hidden=true href=#2-解决的关键问题与贡献-key-problem-solved--contribution>#</a></h2><h3 id=21-解决的关键技术问题>2.1 解决的关键技术问题<a hidden class=anchor aria-hidden=true href=#21-解决的关键技术问题>#</a></h3><p>如何改造输入信息，使得经典且强大的 Bundle Adjustment 优化框架能够直接、统一地处理包含大量动态物体的场景，从而在不丢弃动态信息的前提下，同时实现高精度的相机位姿和时序一致的密集场景重建？</p><h3 id=22-核心贡献>2.2 核心贡献<a hidden class=anchor aria-hidden=true href=#22-核心贡献>#</a></h3><ol><li><strong>核心思想：运动解耦（Motion Decoupling）</strong>：这是本文的灵魂。作者提出，任何一个点的观测运动（total motion）都可以被分解为<strong>相机运动引发的静态分量</strong>（static component）和<strong>物体自身运动引发的动态分量</strong>（dynamic component）。通过只把这个“静态分量”喂给 BA，BA 就可以像处理静态点一样处理动态点，从而统一了整个优化过程。</li><li><strong>双追踪器架构（Dual-Tracker Architecture）</strong>：为了准确地实现运动解耦，做着设计了一个由两个 Transformer 追踪器组成的 Front-end。一个追踪器 <code>$T$</code> 负责预测总运动 <code>$\mathbf{X}_{\text{total}}$</code>，另一个更小更高效的追踪器 <code>${T}_{\text{dyn}}$</code> 专门预测动态分量 <code>$\mathbf{X}_{\text{dyn}}$</code>。这个设计在实验中被证明优于单一追踪器直接预测静态分量的方案。</li><li><strong>统一框架 BA-Track</strong>：提出了一个由三个阶段组成的完整 pipeline：<ul><li>Stage 1: 基于学习的<strong>运动解耦 3D 追踪器</strong>，分离出运动的静态分量。</li><li>Stage 2: 经典的 <strong>Bundle Adjustment</strong>，利用静态运动分量稳健地估计相机位姿和稀疏场景结构。</li><li>Stage 3: 轻量级的全局优化，利用 BA 得到的稀疏精确点来优化初始的密集深度图，得到全局一致的密集重建。</li></ul></li></ol><h2 id=3-方法详述-method>3. 方法详述 (Method)<a hidden class=anchor aria-hidden=true href=#3-方法详述-method>#</a></h2><figure class=align-center><img loading=lazy src=images/BA-Track_framework.jpg#center width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 1. BA-Track Framework</p></figcaption></figure><h3 id=31-stage-i-运动解耦的3d追踪器-motion-decoupled-3d-tracker>3.1 Stage I: 运动解耦的3D追踪器 (Motion-Decoupled 3D Tracker)<a hidden class=anchor aria-hidden=true href=#31-stage-i-运动解耦的3d追踪器-motion-decoupled-3d-tracker>#</a></h3><p>这一阶段是整个框架的基石。它的核心目标不是简单地追踪点，而是要从任何一个点的总观测运动中，精准地<strong>分离出纯粹由相机运动引起的静态分量</strong>（static component）。</p><p>这个思想在图 2 中非常直观的展示了，对于一个动态物体上的点，我们观测到的总运动 <code>$\mathbf{X}_{\text{total}}$</code>（蓝色向量），实际上是相机自身运动 <code>$\mathbf{X}_{\text{static}}$</code>（橙色向量）和物体自身运动 <code>$\mathbf{X}_{\text{dyn}}$</code>（紫色向量）的叠加。</p><figure class=align-center><img loading=lazy src=images/motion_decoupling.jpg#center width=70% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 2. Illustration of motion decoupling</p></figcaption></figure><p>为了实现这个解耦，作者设计了一个巧妙的 Front-end：</p><ol><li><strong>双追踪器架构 (Dual-Tracker Architecture)</strong>: 作者没有用一个网络去硬解这个难题，而是用了两个基于 Transformer 的追踪器。<ul><li><strong>主追踪器 <code>$T$</code></strong>: 负责预测点的<strong>总观测运动</strong> <code>$\mathbf{X}_{\text{total}}$</code>，同时还预测该点的可见性 <code>$v$</code> 和一个动态标签 <code>$m$</code>（一个0到1之间的软标签，表示该点属于动态物体的概率）。</li><li><strong>动态追踪器 <code>$T_{\text{dyn}}$</code></strong>: 这是一个更小、更高效的网络（层数是 <code>$T$</code> 的一半），专门负责预测物体自身的动态运动分量 <code>$\mathbf{X}_{\text{dyn}}$</code>。</li><li><strong>为什么用两个</strong>？论文在消融实验中证明，让一个网络同时学习追踪和理解复杂的运动模式是次优的。通过将任务分解给两个网络，每个网络可以更专注，从而实现更准确的解耦。</li></ul></li><li><strong>运动解耦公式</strong>:</li></ol><div>$$
\mathbf{X}_{\text{static}} = \mathbf{X}_{\text{total}} - m \cdot \mathbf{X}_{\text{dyn}} \tag{1}
$$</div><ul><li>如果一个点是静态的（<code>$m$</code> 趋近于0），那么 <code>$\mathbf{X}_{\text{static}}$</code> 就约等于 <code>$\mathbf{X}_{\text{total}}$</code>，这符合事实。</li><li>如果一个点是动态的（<code>$m$</code> 趋近于1），那么它的静态分量 <code>$\mathbf{X}_{\text{static}}$</code> 就是其总运动减去物体自身运动。这样一来，即使是动态物体上的点，其 <code>$\mathbf{X}_{\text{static}}$</code> 也只反映了相机的运动，使得它在 BA 看来和一个静态点是等价的。</li></ul><h3 id=32-stage-ii-bundle-adjustment-ba>3.2 Stage II: Bundle Adjustment (BA)<a hidden class=anchor aria-hidden=true href=#32-stage-ii-bundle-adjustment-ba>#</a></h3><p>利用第一阶段“净化”后的运动信息 <code>$\mathbf{X}_{\text{static}}$</code>，运行一个鲁棒的 BA，以高精度地恢复相机位姿和场景的稀疏三维结构。</p><p>BA 是一个经典的优化问题，通过最小化重投影误差来同时优化相机参数和三维点坐标。本作的关键在于，它将 BA 的输入进行了“改造”。</p><ol><li><strong>“BA友好”的输入</strong>: BA 处理的不再是原始的、带有动态噪声的观测点，而是 Front-end 输出的运动静态分量 <code>$\mathbf{X}_{\text{static}}$</code>。这使得经典的BA框架几乎无需修改就可以直接应用于动态场景，因为动态物体的“干扰”已经在输入端被消除了。</li><li><strong>优化目标</strong>: $$\argmin_{{\mathbf{T}_t},{\mathbf{Y}}} \sum_{|i-j|\leq S} \sum_n W^i_n(j) | \mathcal{P}_j(\mathbf{x}^i_n, y^i_n) - X^t_n(j) |_\rho + \alpha | y^i_n - d(\mathbf{X}^i_n) |^2 \tag{2}$$<ul><li><strong>优化变量</strong>: 所有帧的相机位姿 <code>$\{\mathbf{T}_t\}$</code> 和所有稀疏查询点的精确深度 <code>$\{\mathbf{Y}\}$</code>。</li><li><strong>第一项：重投影误差</strong>: <code>$\mathcal{P}_j$</code> 是标准的投影函数，它将第 <code>$i$</code> 帧的点 <code>$\mathbf{x}^i_n$</code> 根据当前的相机位姿估计 <code>$\{\mathbf{T}_t\}$</code> 投影到第 <code>$j$</code> 帧。误差是这个投影点与 Front-end 输出的追踪点 <code>$X^i_n(j)$</code> 之间的距离。</li><li><strong>第二项：深度一致性</strong>: 这是一个正则项，它鼓励优化后的深度 <code>$y_n^i$</code> 不要偏离初始的深度先验 <code>$d(\mathbf{X}^i_n)$</code> 太远，由超参数 <code>$\alpha$</code> 控制权重。</li><li><strong>权重 <code>$W$</code></strong>: 权重 <code>$W_n^i(j)$</code> 包含了可见性 <code>$v$</code> 和动态标签 <code>$m$</code> 的信息，形式为 <code>$v \cdot (1-m)$</code>。这意味着，在优化位姿时，BA 会更相信那些可见且被判断为静态的点，从而进一步提升了相机位姿估计的鲁棒性。</li></ul></li></ol><h3 id=33-stage-iii-全局优化-global-refinement>3.3 Stage III: 全局优化 (Global Refinement)<a hidden class=anchor aria-hidden=true href=#33-stage-iii-全局优化-global-refinement>#</a></h3><p>BA 只输出了精确的相机位姿和<strong>稀疏</strong>的三维点。而初始的深度图是<strong>密集</strong>但可能不准确且时序不一致的。此阶段的目标就是利用 BA 输出的精确稀疏点，来对齐和优化整个视频的密集深度图，得到全局一致的密集重建结果。</p><p>作者没有直接对密集的深度图进行复杂变形，而是采用了一种更高效的 scale map 方法。</p><ol><li><strong>可变形的 scale map</strong>: 对每一帧，模型学习一个比原图分辨率低的 2D scale map <code>$\theta_t$</code></li><li><strong>深度优化公式</strong>: 最终的优化后深度由一个简单的乘法得到
$$
\hat{D}_t[\mathbf{x}] = \theta_t[\mathbf{x}]\cdot D_t[\mathbf{x}]
$$
其中 <code>$D_t[\mathbf{x}]$</code> 是初始深度，<code>$\theta_t[\mathbf{x}]$</code> 是从低分辨率 scale map 中通过双线性插值得到的缩放因子。这意味着整个优化过程是学习一个平滑的、空间可变的缩放场来校正初始深度。</li><li><strong>两个核心损失函数</strong>:<ul><li>$\mathcal{L}_\text{depth}$: 深度一致性损失。它强制要求，在稀疏点的那些位置上，优化后的密集深度 <code>$\hat{D}_t$</code> 应该要等于 BA 阶段得到的精确稀疏深度。这相当于用精确的稀疏点作为“锚点”来校准整个密集深度场。</li><li>$\mathcal{L}_\text{rigid}$: 场景刚性损失，它要求场景中<strong>静态部分</strong>的任意两点之间的三维距离，在不同帧之间应该保持不变。这个损失通过一个权重 <code>$W_\text{static}$</code> 来确保只对静态点生效，从而防止了静态背景在优化过程中发生不应有的形变。</li></ul></li></ol><p>通过这三个阶段的紧密协作，BA-Track 成功地将经典 BA 的强大优化能力引入到充满挑战的动态世界中，实现了非常出色的效果。</p><h2 id=4-实验分析-experiments>4. 实验分析 (Experiments)<a hidden class=anchor aria-hidden=true href=#4-实验分析-experiments>#</a></h2><h3 id=41-camera-pose-estimation>4.1 Camera Pose Estimation<a hidden class=anchor aria-hidden=true href=#41-camera-pose-estimation>#</a></h3><figure class=align-center><img loading=lazy src=images/tab1.jpg#center width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 1. Camera pose evaluation results on Sintel, Shibuya, and Epic Fields datasets</p></figcaption></figure><figure class=align-center><img loading=lazy src=images/cam_pose.jpg#center width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 3. Qualitative camera pose estimation results on Sintel, Shibuya, and Epic Fields datasets</p></figcaption></figure><p>如图 3 所示，直观展示了 BA-Track 相机轨迹相比其他方法更平滑、更接近 ground truth，而其他方法在这些场景下往往表现不佳。在具有快速动态内容和复杂相机运动的 Epic Fields 数据集上，BA-Track 仍能保持出色的 VO accuracy。</p><h3 id=42-depth-evaluation>4.2 Depth Evaluation<a hidden class=anchor aria-hidden=true href=#42-depth-evaluation>#</a></h3><figure class=align-center><img loading=lazy src=images/tab2.jpg#center width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 2. Depth evaluation results on Sintel, Shibuya, and Bonn datasets</p></figcaption></figure><p>基于尺度地图的优化方法在参数较少的情况下实现了高效的优化，同时保持了相对简单的结构。</p><h3 id=43-motion-decoupling>4.3 Motion Decoupling<a hidden class=anchor aria-hidden=true href=#43-motion-decoupling>#</a></h3><figure class=align-center><img loading=lazy src=images/motion_de_vis.jpg#center width=80% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 4. Motion decoupling on the DAVIS dataset</p></figcaption></figure><p>图 4 展示了在 DAVIS 数据集上观测到的点运动与静态点运动情况。红色点表示动态轨迹，绿色点代表静态点，突显了三维追踪器分离观测运动中动态成分的能力。</p><h3 id=44-ablation-study>4.4 Ablation Study<a hidden class=anchor aria-hidden=true href=#44-ablation-study>#</a></h3><h4 id=441-dual-tracker-architecture>4.4.1 Dual-Tracker Architecture<a hidden class=anchor aria-hidden=true href=#441-dual-tracker-architecture>#</a></h4><figure class=align-center><img loading=lazy src=images/tab3.jpg#center width=50% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 3. Ablation study of dynamic handling methods on Sintel</p></figcaption></figure><h4 id=442-global-refinement>4.4.2 Global Refinement<a hidden class=anchor aria-hidden=true href=#442-global-refinement>#</a></h4><figure class=align-center><img loading=lazy src=images/tab4.jpg#center width=50% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 4. Ablation study on depth refinement on Bonn crowd2 sequence</p></figcaption></figure><p>分别尝试关闭 <code>$\mathcal{L}_\text{depth}$</code> 和 <code>$\mathcal{L}_\text{rigid}$</code> 进行联合关闭与交替关闭的实验。完全移除两项损失函数会导致重建精度下降。</p><figure class=align-center><img loading=lazy src=images/global_refine.jpg#center width=70% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 5. Visualization of global refinement on the DAVIS</p></figcaption></figure><p>图 5 展示了采用与不采用我们深度优化方法进行重建的两种直观对比。直接将单目深度图与估计的相机位姿融合，会导致重建结果不一致，并出现重复的三维结构，而我们的全局优化显著提升了三维一致性。</p><h2 id=5-批判性思考-critical-analysis--personal-thoughts>5. 批判性思考 (Critical Analysis & Personal Thoughts)<a hidden class=anchor aria-hidden=true href=#5-批判性思考-critical-analysis--personal-thoughts>#</a></h2><h3 id=51-优点-strengths>5.1 优点 (Strengths)<a hidden class=anchor aria-hidden=true href=#51-优点-strengths>#</a></h3><ul><li><strong>思想极其优雅 (Conceptual Elegance)</strong>: 本文的核心 idea 非常漂亮。它没有去硬碰硬地为动态物体建模，而是通过“运动解耦”四两拨千斤，让经典的 BA 工具重获新生。这种解决问题的思路本身就极具启发性，是顶会 Oral 的典范。</li><li><strong>效果非常扎实</strong>: 不仅提出了新颖的理论，更在多个高难度动态场景基准上取得了 SOTA 的性能，用实验结果充分证明了想法的有效性。</li><li>完美的混合系统: 它是“传统几何优化”与“深度学习感知”成功结合的又一力作。用学习的追踪器来做它擅长的事（提供鲁棒的对应和运动先验），用经典的 BA 来做它擅长的事（高精度、有几何约束的优化），强强联合。</li></ul><h3 id=52-潜在缺点疑问-weaknessesquestions>5.2 潜在缺点/疑问 (Weaknesses/Questions)<a hidden class=anchor aria-hidden=true href=#52-潜在缺点疑问-weaknessesquestions>#</a></h3><ul><li><strong>对深度先验的依赖</strong>: 整个 pipeline 的起点是第三方的单目深度估计网络。如果初始深度质量很差（例如在透明、反光物体上），可能会影响第一阶段运动解耦的准确性，进而误差会传导至后续的 BA 和全局优化。</li><li><strong>运动解耦的鲁棒性</strong>: 核心公式 <code>$\mathbf{X}_\text{static} = \mathbf{X}_\text{total} - m \cdot \mathbf{X}_\text{dyn}$</code> 的成败，高度依赖于动态标签 <code>$m$</code> 和动态运动 <code>$\mathbf{X}_\text{dyn}$</code> 的预测精度。当动态和静态界限模糊，或物体运动模式罕见时，Front-end 的预测错误可能会对最终结果产生较大影响。</li><li><strong>对相机内参的假设</strong>: 论文中假设相机内参是已知的 。虽然作者在附录中讨论了未来可以联合优化内参，但这在当前版本中仍是一个限制。</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://gavinsun0921.github.io/tags/point-tracking/>Point Tracking</a></li><li><a href=https://gavinsun0921.github.io/tags/4d-reconstruction/>4D Reconstruction</a></li><li><a href=https://gavinsun0921.github.io/tags/computer-vision/>Computer Vision</a></li></ul><nav class=paginav><a class=prev href=https://gavinsun0921.github.io/posts/fast-paper-reading-06/><span class=title>« Prev</span><br><span>[ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告</span>
</a><a class=next href=https://gavinsun0921.github.io/posts/fast-paper-reading-04/><span class=title>Next »</span><br><span>[ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy 阅读报告</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告 on x" href="https://x.com/intent/tweet/?text=%5bICCV%2725%20Oral%5d%20Back%20on%20Track%3a%20Bundle%20Adjustment%20for%20Dynamic%20Scene%20Reconstruction%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-05%2f&amp;hashtags=PointTracking%2c4DReconstruction%2cComputerVision"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-05%2f&amp;title=%5bICCV%2725%20Oral%5d%20Back%20on%20Track%3a%20Bundle%20Adjustment%20for%20Dynamic%20Scene%20Reconstruction%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;summary=%5bICCV%2725%20Oral%5d%20Back%20on%20Track%3a%20Bundle%20Adjustment%20for%20Dynamic%20Scene%20Reconstruction%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;source=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-05%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-05%2f&title=%5bICCV%2725%20Oral%5d%20Back%20on%20Track%3a%20Bundle%20Adjustment%20for%20Dynamic%20Scene%20Reconstruction%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-05%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告 on whatsapp" href="https://api.whatsapp.com/send?text=%5bICCV%2725%20Oral%5d%20Back%20on%20Track%3a%20Bundle%20Adjustment%20for%20Dynamic%20Scene%20Reconstruction%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a%20-%20https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-05%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告 on telegram" href="https://telegram.me/share/url?text=%5bICCV%2725%20Oral%5d%20Back%20on%20Track%3a%20Bundle%20Adjustment%20for%20Dynamic%20Scene%20Reconstruction%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-05%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 阅读报告 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bICCV%2725%20Oral%5d%20Back%20on%20Track%3a%20Bundle%20Adjustment%20for%20Dynamic%20Scene%20Reconstruction%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-05%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=GavinSun0921/gavinsun0921.github.io data-repo-id=R_kgDOJgiWSg data-category=Announcements data-category-id=DIC_kwDOJgiWSs4CtrHs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://gavinsun0921.github.io/>Gavin Sun · Spatial Intelligence</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>