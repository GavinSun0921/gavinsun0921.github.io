<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读 | Gavin's Home</title><meta name=keywords content="Diffusion,DPM,Computer Vision,Image Generation,Deep Learning"><meta name=description content="This paper introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. And it is important to learn Score-Based generative network and Ito diffusion SDE."><meta name=author content><link rel=canonical href=https://gavinsun0921.github.io/posts/fast-paper-reading-03/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://gavinsun0921.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gavinsun0921.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gavinsun0921.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gavinsun0921.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gavinsun0921.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gavinsun0921.github.io/posts/fast-paper-reading-03/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredTags:["script","noscript","style","textarea","pre"],throwOnError:!1})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-18P7N9RZDS"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-18P7N9RZDS")</script><meta property="og:url" content="https://gavinsun0921.github.io/posts/fast-paper-reading-03/"><meta property="og:site_name" content="Gavin's Home"><meta property="og:title" content="[NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读"><meta property="og:description" content="This paper introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. And it is important to learn Score-Based generative network and Ito diffusion SDE."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-25T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-25T00:00:00+00:00"><meta property="article:tag" content="Diffusion"><meta property="article:tag" content="DPM"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="[NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读"><meta name=twitter:description content="This paper introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. And it is important to learn Score-Based generative network and Ito diffusion SDE."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gavinsun0921.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读","item":"https://gavinsun0921.github.io/posts/fast-paper-reading-03/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读","name":"[NeurIPS\u002719 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读","description":"This paper introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. And it is important to learn Score-Based generative network and Ito diffusion SDE.","keywords":["Diffusion","DPM","Computer Vision","Image Generation","Deep Learning"],"articleBody":"Overview This paper introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. And it is important to learn Score-Based generative network and Ito diffusion SDE. In this paper, the training and inference phases are analyzed separately and solutions are proposed for different problems. Different levels of noise are used during training to overcome the problem that gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. The models in this paper produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10.\nScore-based Generative Modeling Defination of Score Suppose our dataset consists of i.i.d. samples $\\{ \\mathbf{x}_i \\in \\mathbb{R}^D \\} _ {i=1} ^N$ from an unknown data distribution $p_\\text{data}(\\mathbf{x})$.\nWe define the score of a probability density $p(\\mathbf{x})$ to be $\\nabla_\\mathbf{x}\\log p(\\mathbf{x})$. The score network $\\mathbf{s}_\\mathbf{\\theta}:\\mathbb{R}^D \\to \\mathbb{R}^D$ is a neural network parameterized by $\\mathbf{\\theta}$, which will be trained to approximate the score of $p_\\text{data}(\\mathbf{x})$ The goal of generative modeling is to use the dataset to learn a model for generating new samples from $p_\\text{data}(\\mathbf{x})$. The framework of score-based generative modeling has two ingredients: score matching and Langevin dynamics.\nScore Matching for Score Estimation Score matching (Aapo Hyvärinen, 2005) is originally designed for learning non-normalized statistical models based on i.i.d. samples from an unknown data distribution. Following Song et al. (2019), authors repurpose it for score estimation. Using score matching, authors can directly train a score network $\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})$ to estimate $\\nabla_\\mathbf{x}\\log p_\\text{data}(\\mathbf{x})$ without training a model to estimate $p_\\text{data}(\\mathbf{x})$ first. Different from the typical usage of score matching, authors opt not to use the gradient of an energy-based model as the score network to avoid extra computation due to higher-order gradients.\nThe objective minimizes $\\frac{1}{2}\\mathbb{E}_{p_\\text{data}(\\mathbf{x})}\\left[\\left \\| \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}) - \\nabla_\\mathbf{x} \\log p_\\text{data}(\\mathbf{x}) \\right \\|^2_2 \\right]$, which can be shown equivalent to the following up to a constant $$ \\frac{1}{2}\\mathbb{E}_{p_\\text{data}(\\mathbf{x})}\\left[ \\text{tr}(\\nabla _\\mathbf{x} \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})) + \\frac{1}{2} \\left \\| \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}) \\right \\|^2_2 \\right] \\tag{1} $$ where $\\nabla _\\mathbf{x} \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})$ denotes the Jacobian of $\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})$. However, score matching is not scalable to deep networks and high dimensional data due to the computation of $\\text{tr}(\\nabla _\\mathbf{x} \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}))$. Below authors discuss two popular methods for large scale score mathing.\nDenoising Score Mathcing This is the main method used by the authors in the methodology below.\nDenoising score mathcing (Pascal Vincent, 2011) is a variant of score matching that completely circumvents $\\text{tr}(\\nabla _\\mathbf{x} \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}))$. It first perturbs the data point $\\mathbf{x}$ with a pre-specified noise distribution $q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x})$ and then employs score matching to estimate the score of the perturbed data distribution $q_\\sigma(\\tilde{\\mathbf{x}}) \\triangleq \\int q_\\sigma (\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) \\mathrm{d}\\mathbf{x}$. The objective was proved equivalent to the following: $$ \\frac{1}{2}\\mathbb{E}_{q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) p_\\text{data}(\\mathbf{x}) } \\left [ \\| \\mathbf{s}_\\mathbf{\\theta}(\\tilde{\\mathbf{x}}) - \\nabla_{\\tilde{\\mathbf{x}}} \\log q_\\sigma (\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) \\|^2_2 \\right ] \\tag{2} $$ However, $\\mathbf{s}_\\mathbf{\\theta}^{*} = \\nabla_\\mathbf{x} \\log q_\\sigma(\\mathbf{x}) \\approx \\nabla_\\mathbf{x} \\log p_{\\text{data}}(\\mathbf{x})$ is true only when the noise is small enough such that $q_\\sigma(\\mathbf{x}) \\approx p_\\text{data}(\\mathbf{x})$.\nSliced Score Matching Sliced score matching (Song et al. 2019) uses random projections to approximate $\\text{tr}(\\nabla _\\mathbf{x} \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}))$ in score matching. The objective is $$ \\mathbb{E}_{p_\\mathbf{v}}\\mathbb{E}_{p_\\text{data}} \\left [ \\mathbf{v}^\\top \\nabla_\\mathbf{x}\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})\\mathbf{v} + \\frac{1}{2} \\| \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}) \\|^2_2 \\right ] \\tag{3} $$ where $p_\\mathbf{v}$ is a simple ditribution of random vectors, e.g., the multivariate standard normal. The term $\\mathbf{v}^\\top \\nabla_\\mathbf{x}\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})\\mathbf{v}$ can be efficiently computed by forward mode auto-differentiation. Unlike denoising score matching which estimates the scores of perturbed data, sliced score matching provides score estimation for the original unperturbed data distribution, but requires around four times more computations due to the forward mode auto-differentiation.\nSampling with Langevin Dynamics Langevin dynamics can produce samples from a probability density $p(\\mathbf{x})$ using only the score function $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$. Given a fixed step size $\\epsilon \u003e 0$, and an initial value $\\tilde{\\mathbf{x}}_0 \\sim \\pi(\\mathbf{x})$ with $\\pi$ being a prior distribution (arbitrary), the Langevin method recursively computes the following $$ \\tilde{\\mathbf{x}}_t = \\tilde{\\mathbf{x}}_{t-1} + \\frac{\\epsilon}{2} \\nabla_\\mathbf{x} \\log p(\\tilde{\\mathbf{x}}_{t-1}) + \\sqrt{\\epsilon} \\mathbf{z}_t \\tag{4} $$ where $\\mathbf{z}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The distribution of $\\tilde{\\mathbf{x}}_T$ equals $p(\\mathbf{x})$ when $\\epsilon \\to 0$ and $T \\to \\infin$, in which case $\\tilde{x}_T$ becomes an exact sample from $p(\\mathbf{x})$ under some regularity conditions. When $\\epsilon \u003e 0$ and $T \u003c \\infin$, a Metropolis-Hastings update is needed to correct the error of Eq. (4), but it can often be ignored in practice. In this work, authors assume this error is negligible when $\\epsilon$ is small and $T$ is large.\nThe authors give the goals and reasons for network modeling. Sampling from Eq. (4) only requires the score function $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$. Therefore, in order to obtain samples from $p_\\text{data}(\\mathbf{x})$, authors first train score network such that $\\mathbf{s}_\\theta(\\mathbf{x}) \\approx \\nabla_\\mathbf{x} \\log p_\\text{data}(\\mathbf{x})$ and then approximately obtain samples with Langevin dynamics using $\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x})$. This is the key idea of our framework of score-based generative modeling.\nChallenges of Low Data Density Regions In regions of low data density, score matching may not have enough evidence to estimate score functions accurately, due to the lack of data samples. When sampling with Langevin dynamics, our initial sample is highly likely in low density regions when data reside in a high dimensional space. Therefore, having an inaccurate score-based model will derail Langevin dynamics from the very beginning of the procedure, preventing it from generating high quality samples that are representative of the data.\nFig. 1. Estimated scores are only accurate in high density regions. (Image source: Yang Song’ blog, 2021)\nAuthors solution is to perturb data points with noise and train score-based models on the noisy data points instead. When the noise magnitude is sufficiently large, it can populate low data density regions to improve the accuracy of estimated scores. For example, here is what happens when we perturb a mixture of two Gaussians perturbed by additional Gaussian noise.\nFig. 2. Estimated scores are accurate everywhere for the noise-perturbed data distribution due to reduced low data density regions. (Image source: Yang Song’ blog, 2021)\nYet another question remains: how to choose an appropriate noise scale for the perturbation process? Larger noise can obviously cover more low density regions for better score estimation, but it over-corrupts the data and alters it significantly from the original distribution. Smaller noise, on the other hand, causes less corruption of the original data distribution, but does not cover the low density regions as well as we would like. To achieve the best of both worlds, authors use multiple scales of noise perturbations simultaneously.\nNoise Conditional Score Networks Perturbing the data using various levels of noise; Simultaneously estimating scores corresponding all noise levels by training a single conditional score network. After training, when using Langevin dynamics to generate samples, we initially use scores corresponding to large noise, and gradually anneal down the noise level. Note that conditional in NCSN is for noise and remains unconditional for the image generation task. Define Noise Condtional Score Networks Let $ \\{ \\sigma_i \\} _{i=1}^L $ be a positive geometric sequence that satisfies $\\frac{\\sigma_1}{\\sigma_2} = \\cdots = \\frac{\\sigma_{L-1}}{\\sigma_{L}} \u003e 1$.\nLet $q_\\sigma(\\mathbf{x}) \\triangleq \\int p_\\text{data}(\\mathbf{t}) \\mathcal{N}(\\mathbf{x} \\mid \\mathbf{t}, \\sigma^2 \\mathbf{I}) \\mathrm(d) \\mathbf{t}$ denote the perturbed data distribution.\nAuthors choose the noise levels $\\{ \\sigma_i \\}_{i=1}^L$ such that $\\sigma_1$ is large enough to mitigate the difficulties discussed in Eq. (4), and $\\sigma_L$ is small enough to minimize the effect on data.\nAuthors aim to train a conditional score network to jointly estimate the scores of all perturbed data distributions, i.e., $\\forall \\sigma \\in \\{ \\sigma_i \\}_{i=1}^L : \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}, \\sigma) \\approx \\nabla_\\mathbf{x} \\log q_\\sigma(\\mathbf{x})$. Note that $\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}, \\sigma) \\in \\mathbb{R}^D$ when $\\mathbf{x} \\in \\mathbb{R}^D$. Authors call $\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}, \\sigma)$ a Noise Conditional Score Network (NCSN).\nTraining NCSNs via score matching Both sliced and denoising score matching can train NCSNs. Authors adopt denoising score matching as it is slightly faster and naturally fits the task of estimating scores of noise-perturbed data distributions.\nAuthors choose the noise distribution to be $q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) = \\mathcal{N}(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}, \\sigma^2\\mathbf{I})$; therefore $\\nabla_{\\tilde{\\mathbf{x}}} \\log q_\\sigma (\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) = - \\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma^2}$. For a given $\\sigma$, the denoising score matching objective is $$ \\ell(\\mathbf{\\theta; \\sigma}) \\triangleq \\frac{1}{2} \\mathbb{E}_{p_\\text{data}(\\mathbf{x})} \\mathbb{E}_{\\tilde{\\mathbf{x}} \\sim \\mathcal{N}(\\mathbf{x}, \\sigma^2\\mathbf{I})} \\left [ \\left \\| \\mathbf{s}_\\mathbf{\\theta}(\\tilde{\\mathbf{x}}, \\sigma) + \\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma^2} \\right \\|^2_2 \\right ] \\tag{5} $$ Then, author combine Eq. (5) for all $\\sigma \\in \\{ \\sigma_i \\}_{i=1}^L$ to get one unified objective $$ \\mathcal{L}(\\mathbf{\\theta}; \\{ \\sigma_i \\}_{i=1}^L) \\triangleq \\frac{1}{L} \\sum_{i=1}^L \\lambda(\\sigma_i) \\ell(\\mathbf{\\theta; \\sigma_i}) \\tag{6} $$ where $\\lambda(\\sigma_i) \u003e 0$ is a coefficient function depending on $\\sigma_i$. Assuming $\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}, \\sigma)$ has enough capacity, $\\mathbf{s}_\\mathbf{\\theta}^*(\\mathbf{x}, \\sigma)$ minimizes Eq. (6) iff $\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}, \\sigma_i) = \\nabla_\\mathbf{x} \\log q_{\\sigma_i}(\\mathbf{x})$ a.s. for all $i \\in \\{ 1, 2, \\cdots, L \\}$, because Eq. (6) is a conical combination of $L$ denoising score matching objectives.\niff: if and only if a.s.: almost surely There can be many possible choices of $\\lambda(\\cdot)$. Ideally, authors hope that the values of $\\lambda(\\sigma_i)\\ell(\\mathbf{\\theta};\\sigma_i)$ for all $\\{ \\sigma_i \\}_{i=1}^L$ are roughly of the same order of magnitude. Empirically, we observe that when the score networks are trained to optimality, authors approximately have $\\| \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}, \\sigma) \\|_2 \\propto \\frac{1}{\\sigma}$. This inspires authors to choose $\\lambda(\\sigma) = \\sigma^2$. Because under this choice, there is $\\lambda(\\sigma)\\ell(\\mathbf{\\theta};\\sigma) = \\sigma^2 \\ell(\\mathbf{\\theta}; \\sigma) = \\frac{1}{2} \\mathbb{E} [ \\| \\sigma \\mathbf{s}_\\mathbf{\\theta}(\\tilde{\\mathbf{x}}, \\sigma) + \\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma} \\|_2^2 ]$. Since $\\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ and $\\| \\sigma \\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}, \\sigma) \\|_2 \\propto 1$, authors conclude that the order of magnitude of $\\lambda(\\sigma)\\ell(\\mathbf{\\theta};\\sigma)$ does not depend on $\\sigma$.\nWhat specific benefit this has it not stated by the authors in the original article, but I think it should be to standardize the magnitude for different levels of noise, so that a single loss function (Eq. (5)) after perturbation of the data by different levels of noise will have the same weight in the overall loss function (Eq. (6)), i.e., the supervisory weights for matching scores to the data after perturbation of all levels of noise at training time are equal.\nNCSN inference via annealed Langevin dynamics After the NCSN $\\mathbf{s}_\\mathbf{\\theta}(\\mathbf{x}, \\sigma)$ is trained, authors proposed a sampling approach—annealed Langevin dynamics (Fig. 3).\nFig. 3. Algorithm of annealed Langevin dynamics. (Algorithm source: Song \u0026 Ermon, 2019)\nThis algorithm is inspired by simulated annealing and annealed importance sampling. This algorithm start annealed Langevin dynamics by initializing the samples from some fixed prior distribution, e.g., uniform noise. Then run Langevin dynamics to sample from $q_{\\sigma_1}(\\mathbf{x})$ with step size $\\alpha_1$. Next run Langevin dynamics to sample $q_{\\sigma_2}(\\mathbf{x})$, starting from the final samples of the previous simulation and using a reduced step size $\\alpha_2$. Authors continue in this fashion, using the final samples of Langevin dynamics for $q_{\\sigma_{i-1}}(\\mathbf{x})$ as the initial samples of Lnagevin dynamic for $q_{\\sigma_i}(\\mathbf{x})$, and tuning down the step size $\\alpha_i$ gradually with $\\alpha_i = \\epsilon \\cdot \\sigma_i^2 / \\sigma_L^2$. Finnaly, run Langevin dynamics to sample from $q_{\\sigma_L}(\\mathbf{x})$, which is close to $p_\\text{data}(\\mathbf{x})$ when $\\sigma_L \\approx 0$.\nResult and Conclusion The authors conducted quantitative tests with excellent results, but of more interest in this article is the theoretical foundation of the Score-Based Generative Model, and much of the knowledge and assumptions in this article were utilized in Yang Songs subsequent diffusion work.\nAs an unconditional model, we achieve the state-of-the-art inception score of 8.87, which is even better than most reported values for class-conditional generative models.\nTable. 1. Inception and FID scores for CIFAR-10. (Table source: Song \u0026 Ermon, 2019)\nReferences [1] Yang Song \u0026 Stefano Ermon. “Generative Modeling by Estimating Gradients of the Data Distribution.” NeurIPS 2019.\n[2] Aapo Hyvärinen. “Estimation of Non-Normalized Statistical Models by Score Matching.” JMLR 2005.\n[3] Yang Song et al. “Sliced Score Matching: A Scalable Approach to Density and Score Estimation.” Uncertainty in Artificial Intelligence 2019.\n[4] Pascal Vincent. “A Connection Between Score Matching and Denoising Autoencoders.” Neural Computation 2011.\n","wordCount":"1992","inLanguage":"en","datePublished":"2023-08-25T00:00:00Z","dateModified":"2023-08-25T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://gavinsun0921.github.io/posts/fast-paper-reading-03/"},"publisher":{"@type":"Organization","name":"Gavin's Home","logo":{"@type":"ImageObject","url":"https://gavinsun0921.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gavinsun0921.github.io/ accesskey=h title="Gavin's Home (Alt + H)">Gavin's Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gavinsun0921.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://gavinsun0921.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gavinsun0921.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://gavinsun0921.github.io/about/ title=AboutMe><span>AboutMe</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读</h1><div class=post-meta><span title='2023-08-25 00:00:00 +0000 UTC'>August 25, 2023</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;1992 words</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#overview aria-label=Overview>Overview</a></li><li><a href=#score-based-generative-modeling aria-label="Score-based Generative Modeling">Score-based Generative Modeling</a><ul><li><a href=#defination-of-score aria-label="Defination of Score">Defination of Score</a></li><li><a href=#score-matching-for-score-estimation aria-label="Score Matching for Score Estimation">Score Matching for Score Estimation</a><ul><li><a href=#denoising-score-mathcing aria-label="Denoising Score Mathcing">Denoising Score Mathcing</a></li><li><a href=#sliced-score-matching aria-label="Sliced Score Matching">Sliced Score Matching</a></li></ul></li><li><a href=#sampling-with-langevin-dynamics aria-label="Sampling with Langevin Dynamics">Sampling with Langevin Dynamics</a></li><li><a href=#challenges-of-low-data-density-regions aria-label="Challenges of Low Data Density Regions">Challenges of Low Data Density Regions</a></li><li><a href=#noise-conditional-score-networks aria-label="Noise Conditional Score Networks">Noise Conditional Score Networks</a><ul><li><a href=#define-noise-condtional-score-networks aria-label="Define Noise Condtional Score Networks">Define Noise Condtional Score Networks</a></li><li><a href=#training-ncsns-via-score-matching aria-label="Training NCSNs via score matching">Training NCSNs via score matching</a></li><li><a href=#ncsn-inference-via-annealed-langevin-dynamics aria-label="NCSN inference via annealed Langevin dynamics">NCSN inference via annealed Langevin dynamics</a></li></ul></li><li><a href=#result-and-conclusion aria-label="Result and Conclusion">Result and Conclusion</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p><strong>This paper introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. And it is important to learn Score-Based generative network and Ito diffusion SDE.</strong> In this paper, the training and inference phases are analyzed separately and solutions are proposed for different problems. Different levels of noise are used during training to overcome the problem that gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. The models in this paper produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10.</p><h2 id=score-based-generative-modeling>Score-based Generative Modeling<a hidden class=anchor aria-hidden=true href=#score-based-generative-modeling>#</a></h2><h3 id=defination-of-score>Defination of Score<a hidden class=anchor aria-hidden=true href=#defination-of-score>#</a></h3><p>Suppose our dataset consists of i.i.d. samples $\{ \mathbf{x}_i \in \mathbb{R}^D \} _ {i=1} ^N$ from an unknown data distribution $p_\text{data}(\mathbf{x})$.</p><ul><li>We define the score of a probability density $p(\mathbf{x})$ to be $\nabla_\mathbf{x}\log p(\mathbf{x})$.</li><li>The score network $\mathbf{s}_\mathbf{\theta}:\mathbb{R}^D \to \mathbb{R}^D$ is a neural network parameterized by $\mathbf{\theta}$, which will be trained to approximate the score of $p_\text{data}(\mathbf{x})$</li></ul><p>The goal of generative modeling is to use the dataset to learn a model for generating new samples from $p_\text{data}(\mathbf{x})$.
The framework of score-based generative modeling has two ingredients: <strong>score matching</strong> and <strong>Langevin dynamics</strong>.</p><h3 id=score-matching-for-score-estimation>Score Matching for Score Estimation<a hidden class=anchor aria-hidden=true href=#score-matching-for-score-estimation>#</a></h3><p>Score matching (<a href=https://jmlr.org/papers/v6/hyvarinen05a.html>Aapo Hyvärinen, 2005</a>) is originally designed for learning non-normalized statistical models based on i.i.d. samples from an unknown data distribution. Following <a href=http://auai.org/uai2019/proceedings/papers/204.pdf>Song <em>et al.</em> (2019)</a>, authors repurpose it for score estimation.
<strong>Using score matching, authors can directly train a score network $\mathbf{s}_\mathbf{\theta}(\mathbf{x})$ to estimate $\nabla_\mathbf{x}\log p_\text{data}(\mathbf{x})$ without training a model to estimate $p_\text{data}(\mathbf{x})$ first.</strong> Different from the typical usage of score matching, authors opt not to use the gradient of an energy-based model as the score network to avoid extra computation due to higher-order gradients.</p><p>The objective minimizes $\frac{1}{2}\mathbb{E}_{p_\text{data}(\mathbf{x})}\left[\left \| \mathbf{s}_\mathbf{\theta}(\mathbf{x}) - \nabla_\mathbf{x} \log p_\text{data}(\mathbf{x}) \right \|^2_2 \right]$, which can be shown equivalent to the following up to a constant
$$
\frac{1}{2}\mathbb{E}_{p_\text{data}(\mathbf{x})}\left[ \text{tr}(\nabla _\mathbf{x} \mathbf{s}_\mathbf{\theta}(\mathbf{x})) + \frac{1}{2} \left \| \mathbf{s}_\mathbf{\theta}(\mathbf{x}) \right \|^2_2 \right] \tag{1}
$$
where $\nabla _\mathbf{x} \mathbf{s}_\mathbf{\theta}(\mathbf{x})$ denotes the <span style=background-color:#eea2a4>Jacobian of $\mathbf{s}_\mathbf{\theta}(\mathbf{x})$</span>. <strong>However, score matching is not scalable to deep networks and high dimensional data due to the computation of $\text{tr}(\nabla _\mathbf{x} \mathbf{s}_\mathbf{\theta}(\mathbf{x}))$.</strong> Below authors discuss two popular methods for large scale score mathing.</p><h4 id=denoising-score-mathcing>Denoising Score Mathcing<a hidden class=anchor aria-hidden=true href=#denoising-score-mathcing>#</a></h4><p><em>This is the main method used by the authors in the methodology below.</em></p><p>Denoising score mathcing (<a href=https://ieeexplore.ieee.org/document/6795935>Pascal Vincent, 2011</a>) is a variant of score matching that completely circumvents $\text{tr}(\nabla _\mathbf{x} \mathbf{s}_\mathbf{\theta}(\mathbf{x}))$. It first perturbs the data point $\mathbf{x}$ with a pre-specified noise distribution $q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x})$ and then employs score matching to estimate the score of the perturbed data distribution $q_\sigma(\tilde{\mathbf{x}}) \triangleq \int q_\sigma (\tilde{\mathbf{x}} \mid \mathbf{x}) \mathrm{d}\mathbf{x}$. The objective was proved equivalent to the following:
$$
\frac{1}{2}\mathbb{E}_{q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) p_\text{data}(\mathbf{x}) } \left [ \| \mathbf{s}_\mathbf{\theta}(\tilde{\mathbf{x}}) - \nabla_{\tilde{\mathbf{x}}} \log q_\sigma (\tilde{\mathbf{x}} \mid \mathbf{x}) \|^2_2 \right ] \tag{2}
$$
<strong>However, $\mathbf{s}_\mathbf{\theta}^{*} = \nabla_\mathbf{x} \log q_\sigma(\mathbf{x}) \approx \nabla_\mathbf{x} \log p_{\text{data}}(\mathbf{x})$ is true only when the noise is small enough such that $q_\sigma(\mathbf{x}) \approx p_\text{data}(\mathbf{x})$.</strong></p><h4 id=sliced-score-matching>Sliced Score Matching<a hidden class=anchor aria-hidden=true href=#sliced-score-matching>#</a></h4><p>Sliced score matching (<a href=http://auai.org/uai2019/proceedings/papers/204.pdf>Song <em>et al.</em> 2019</a>) uses random projections to approximate $\text{tr}(\nabla _\mathbf{x} \mathbf{s}_\mathbf{\theta}(\mathbf{x}))$ in score matching. The objective is
$$
\mathbb{E}_{p_\mathbf{v}}\mathbb{E}_{p_\text{data}} \left [ \mathbf{v}^\top \nabla_\mathbf{x}\mathbf{s}_\mathbf{\theta}(\mathbf{x})\mathbf{v} + \frac{1}{2} \| \mathbf{s}_\mathbf{\theta}(\mathbf{x}) \|^2_2 \right ] \tag{3}
$$
where $p_\mathbf{v}$ is a simple ditribution of random vectors, <em>e.g.</em>, the multivariate standard normal. The term $\mathbf{v}^\top \nabla_\mathbf{x}\mathbf{s}_\mathbf{\theta}(\mathbf{x})\mathbf{v}$ can be efficiently computed by forward mode auto-differentiation. <strong>Unlike denoising score matching which estimates the scores of perturbed data, sliced score matching provides score estimation for the original unperturbed data distribution, but requires around four times more computations due to the forward mode auto-differentiation.</strong></p><h3 id=sampling-with-langevin-dynamics>Sampling with Langevin Dynamics<a hidden class=anchor aria-hidden=true href=#sampling-with-langevin-dynamics>#</a></h3><p>Langevin dynamics can produce samples from a probability density $p(\mathbf{x})$ using only the score function $\nabla_\mathbf{x} \log p(\mathbf{x})$. Given a fixed step size $\epsilon > 0$, and an initial value $\tilde{\mathbf{x}}_0 \sim \pi(\mathbf{x})$ with $\pi$ being a prior distribution (arbitrary), the Langevin method recursively computes the following
$$
\tilde{\mathbf{x}}_t = \tilde{\mathbf{x}}_{t-1} + \frac{\epsilon}{2} \nabla_\mathbf{x} \log p(\tilde{\mathbf{x}}_{t-1}) + \sqrt{\epsilon} \mathbf{z}_t \tag{4}
$$
where $\mathbf{z}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. The distribution of $\tilde{\mathbf{x}}_T$ equals $p(\mathbf{x})$ when $\epsilon \to 0$ and $T \to \infin$, in which case $\tilde{x}_T$ becomes an exact sample from $p(\mathbf{x})$ under some regularity conditions. When $\epsilon > 0$ and $T &lt; \infin$, a Metropolis-Hastings update is needed to correct the error of Eq. (4), but it can often be ignored in practice. <strong>In this work, authors assume this error is negligible when $\epsilon$ is small and $T$ is large.</strong></p><p>The authors give the goals and reasons for network modeling. Sampling from Eq. (4) only requires the score function $\nabla_\mathbf{x} \log p(\mathbf{x})$. Therefore, in order to obtain samples from $p_\text{data}(\mathbf{x})$, authors first train score network such that $\mathbf{s}_\theta(\mathbf{x}) \approx \nabla_\mathbf{x} \log p_\text{data}(\mathbf{x})$ and then approximately obtain samples with Langevin dynamics using $\mathbf{s}_\mathbf{\theta}(\mathbf{x})$. <strong>This is the key idea of our framework of score-based generative modeling.</strong></p><h3 id=challenges-of-low-data-density-regions>Challenges of Low Data Density Regions<a hidden class=anchor aria-hidden=true href=#challenges-of-low-data-density-regions>#</a></h3><p>In regions of low data density, score matching may not have enough evidence to estimate score functions accurately, due to the lack of data samples. When sampling with Langevin dynamics, our initial sample is highly likely in low density regions when data reside in a high dimensional space. Therefore, having an inaccurate score-based model will derail Langevin dynamics from the very beginning of the procedure, preventing it from generating high quality samples that are representative of the data.</p><figure class=align-center><img loading=lazy src=images/01.jpg#center alt="Fig. 1. Estimated scores are only accurate in high density regions. (Image source: Yang Song&rsquo; blog, 2021)" width=60%><figcaption><p>Fig. 1. Estimated scores are only accurate in high density regions. (Image source: <a href=https://yang-song.net/blog/2021/score/>Yang Song&rsquo; blog, 2021</a>)</p></figcaption></figure><p>Authors solution is to perturb data points with noise and train score-based models on the noisy data points instead. When the noise magnitude is sufficiently large, it can populate low data density regions to improve the accuracy of estimated scores. For example, here is what happens when we perturb a mixture of two Gaussians perturbed by additional Gaussian noise.</p><figure class=align-center><img loading=lazy src=images/02.jpg#center alt="Fig. 2. Estimated scores are accurate everywhere for the noise-perturbed data distribution due to reduced low data density regions. (Image source: Yang Song&rsquo; blog, 2021)" width=60%><figcaption><p>Fig. 2. Estimated scores are accurate everywhere for the noise-perturbed data distribution due to reduced low data density regions. (Image source: <a href=https://yang-song.net/blog/2021/score/>Yang Song&rsquo; blog, 2021</a>)</p></figcaption></figure><p>Yet another question remains: how to choose an appropriate noise scale for the perturbation process? Larger noise can obviously cover more low density regions for better score estimation, but it over-corrupts the data and alters it significantly from the original distribution. Smaller noise, on the other hand, causes less corruption of the original data distribution, but does not cover the low density regions as well as we would like. <strong>To achieve the best of both worlds, authors use multiple scales of noise perturbations simultaneously.</strong></p><h3 id=noise-conditional-score-networks>Noise Conditional Score Networks<a hidden class=anchor aria-hidden=true href=#noise-conditional-score-networks>#</a></h3><ul><li>Perturbing the data using various levels of noise;</li><li>Simultaneously estimating scores corresponding all noise levels by training a <strong>single</strong> conditional score network.</li><li>After training, when using Langevin dynamics to generate samples, we initially use scores corresponding to large noise, and gradually anneal down the noise level.</li><li><strong>Note that conditional in NCSN is for noise and remains unconditional for the image generation task.</strong></li></ul><h4 id=define-noise-condtional-score-networks>Define Noise Condtional Score Networks<a hidden class=anchor aria-hidden=true href=#define-noise-condtional-score-networks>#</a></h4><p>Let $ \{ \sigma_i \} _{i=1}^L $ be a positive geometric sequence that satisfies $\frac{\sigma_1}{\sigma_2} = \cdots = \frac{\sigma_{L-1}}{\sigma_{L}} > 1$.</p><p>Let $q_\sigma(\mathbf{x}) \triangleq \int p_\text{data}(\mathbf{t}) \mathcal{N}(\mathbf{x} \mid \mathbf{t}, \sigma^2 \mathbf{I}) \mathrm(d) \mathbf{t}$ denote the perturbed data distribution.</p><p>Authors choose the noise levels $\{ \sigma_i \}_{i=1}^L$ such that $\sigma_1$ is large enough to mitigate the difficulties discussed in Eq. (4), and $\sigma_L$ is small enough to minimize the effect on data.</p><p>Authors aim to train a conditional score network to jointly estimate the scores of all perturbed data distributions, <em>i.e.</em>, $\forall \sigma \in \{ \sigma_i \}_{i=1}^L : \mathbf{s}_\mathbf{\theta}(\mathbf{x}, \sigma) \approx \nabla_\mathbf{x} \log q_\sigma(\mathbf{x})$. Note that $\mathbf{s}_\mathbf{\theta}(\mathbf{x}, \sigma) \in \mathbb{R}^D$ when $\mathbf{x} \in \mathbb{R}^D$. Authors call $\mathbf{s}_\mathbf{\theta}(\mathbf{x}, \sigma)$ a <em>Noise Conditional Score Network (NCSN)</em>.</p><h4 id=training-ncsns-via-score-matching>Training NCSNs via score matching<a hidden class=anchor aria-hidden=true href=#training-ncsns-via-score-matching>#</a></h4><p><em>Both sliced and denoising score matching can train NCSNs.</em> Authors adopt denoising score matching as it is slightly faster and naturally fits the task of estimating scores of noise-perturbed data distributions.</p><p>Authors choose the noise distribution to be $q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) = \mathcal{N}(\tilde{\mathbf{x}} \mid \mathbf{x}, \sigma^2\mathbf{I})$; therefore $\nabla_{\tilde{\mathbf{x}}} \log q_\sigma (\tilde{\mathbf{x}} \mid \mathbf{x}) = - \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^2}$. For a given $\sigma$, the denoising score matching objective is
$$
\ell(\mathbf{\theta; \sigma}) \triangleq \frac{1}{2} \mathbb{E}_{p_\text{data}(\mathbf{x})} \mathbb{E}_{\tilde{\mathbf{x}} \sim \mathcal{N}(\mathbf{x}, \sigma^2\mathbf{I})} \left [ \left \| \mathbf{s}_\mathbf{\theta}(\tilde{\mathbf{x}}, \sigma) + \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^2} \right \|^2_2 \right ] \tag{5}
$$
Then, author combine Eq. (5) for all $\sigma \in \{ \sigma_i \}_{i=1}^L$ to get one unified objective
$$
\mathcal{L}(\mathbf{\theta}; \{ \sigma_i \}_{i=1}^L) \triangleq \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \ell(\mathbf{\theta; \sigma_i}) \tag{6}
$$
where $\lambda(\sigma_i) > 0$ is a coefficient function depending on $\sigma_i$. Assuming $\mathbf{s}_\mathbf{\theta}(\mathbf{x}, \sigma)$ has enough capacity, $\mathbf{s}_\mathbf{\theta}^*(\mathbf{x}, \sigma)$ minimizes Eq. (6) iff $\mathbf{s}_\mathbf{\theta}(\mathbf{x}, \sigma_i) = \nabla_\mathbf{x} \log q_{\sigma_i}(\mathbf{x})$ a.s. for all $i \in \{ 1, 2, \cdots, L \}$, because Eq. (6) is a conical combination of $L$ denoising score matching objectives.</p><ul><li><em>iff</em>: if and only if</li><li><em>a.s.</em>: almost surely</li></ul><p>There can be many possible choices of $\lambda(\cdot)$. Ideally, authors hope that the values of $\lambda(\sigma_i)\ell(\mathbf{\theta};\sigma_i)$ for all $\{ \sigma_i \}_{i=1}^L$ are roughly of the same order of magnitude. Empirically, we observe that when the score networks are trained to optimality, authors approximately have $\| \mathbf{s}_\mathbf{\theta}(\mathbf{x}, \sigma) \|_2 \propto \frac{1}{\sigma}$. This inspires authors to choose $\lambda(\sigma) = \sigma^2$. Because under this choice, there is $\lambda(\sigma)\ell(\mathbf{\theta};\sigma) = \sigma^2 \ell(\mathbf{\theta}; \sigma) = \frac{1}{2} \mathbb{E} [ \| \sigma \mathbf{s}_\mathbf{\theta}(\tilde{\mathbf{x}}, \sigma) + \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma} \|_2^2 ]$. Since $\frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ and $\| \sigma \mathbf{s}_\mathbf{\theta}(\mathbf{x}, \sigma) \|_2 \propto 1$, authors conclude that the order of magnitude of $\lambda(\sigma)\ell(\mathbf{\theta};\sigma)$ does not depend on $\sigma$.</p><p>What specific benefit this has it not stated by the authors in the original article, but I think it should be to standardize the magnitude for different levels of noise, so that a single loss function (Eq. (5)) after perturbation of the data by different levels of noise will have the same weight in the overall loss function (Eq. (6)), <em>i.e.</em>, <strong>the supervisory weights for matching scores to the data after perturbation of all levels of noise at training time are equal.</strong></p><h4 id=ncsn-inference-via-annealed-langevin-dynamics>NCSN inference via annealed Langevin dynamics<a hidden class=anchor aria-hidden=true href=#ncsn-inference-via-annealed-langevin-dynamics>#</a></h4><p>After the NCSN $\mathbf{s}_\mathbf{\theta}(\mathbf{x}, \sigma)$ is trained, authors proposed a sampling approach&mdash;annealed Langevin dynamics (Fig. 3).</p><figure class=align-center><img loading=lazy src=images/03.jpg#center alt="Fig. 3. Algorithm of annealed Langevin dynamics. (Algorithm source: Song & Ermon, 2019)" width=90%><figcaption><p>Fig. 3. Algorithm of annealed Langevin dynamics. (Algorithm source: <a href=https://papers.neurips.cc/paper_files/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html>Song & Ermon, 2019</a>)</p></figcaption></figure><p>This algorithm is inspired by simulated annealing and annealed importance sampling. This algorithm start annealed Langevin dynamics by initializing the samples from some fixed prior distribution, <em>e.g.</em>, uniform noise. Then run Langevin dynamics to sample from $q_{\sigma_1}(\mathbf{x})$ with step size $\alpha_1$. Next run Langevin dynamics to sample $q_{\sigma_2}(\mathbf{x})$, starting from the final samples of the previous simulation and using a reduced step size $\alpha_2$. Authors continue in this fashion, using the final samples of Langevin dynamics for $q_{\sigma_{i-1}}(\mathbf{x})$ as the initial samples of Lnagevin dynamic for $q_{\sigma_i}(\mathbf{x})$, and tuning down the step size $\alpha_i$ gradually with $\alpha_i = \epsilon \cdot \sigma_i^2 / \sigma_L^2$. Finnaly, run Langevin dynamics to sample from $q_{\sigma_L}(\mathbf{x})$, which is close to $p_\text{data}(\mathbf{x})$ when $\sigma_L \approx 0$.</p><h3 id=result-and-conclusion>Result and Conclusion<a hidden class=anchor aria-hidden=true href=#result-and-conclusion>#</a></h3><p>The authors conducted quantitative tests with excellent results, <strong>but of more interest in this article is the theoretical foundation of the Score-Based Generative Model, and much of the knowledge and assumptions in this article were utilized in Yang Songs subsequent diffusion work.</strong></p><blockquote><p>As an unconditional model, we achieve the state-of-the-art inception score of 8.87, which is even better than most reported values for class-conditional generative models.</p></blockquote><figure class=align-center><img loading=lazy src=images/04.jpg#center alt="Table. 1. Inception and FID scores for CIFAR-10. (Table source: Song & Ermon, 2019)" width=90%><figcaption><p>Table. 1. Inception and FID scores for CIFAR-10. (Table source: <a href=https://papers.neurips.cc/paper_files/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html>Song & Ermon, 2019</a>)</p></figcaption></figure><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Yang Song & Stefano Ermon. <a href=https://papers.neurips.cc/paper_files/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html>&ldquo;Generative Modeling by Estimating Gradients of the Data Distribution.&rdquo;</a> NeurIPS 2019.</p><p>[2] Aapo Hyvärinen. <a href=https://jmlr.org/papers/v6/hyvarinen05a.html>&ldquo;Estimation of Non-Normalized Statistical Models by Score Matching.&rdquo;</a> JMLR 2005.</p><p>[3] Yang Song <em>et al</em>. <a href=http://auai.org/uai2019/proceedings/papers/204.pdf>&ldquo;Sliced Score Matching: A Scalable Approach to Density and Score Estimation.&rdquo;</a> Uncertainty in Artificial Intelligence 2019.</p><p>[4] Pascal Vincent. <a href=https://ieeexplore.ieee.org/document/6795935>&ldquo;A Connection Between Score Matching and Denoising Autoencoders.&rdquo;</a> Neural Computation 2011.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://gavinsun0921.github.io/tags/diffusion/>Diffusion</a></li><li><a href=https://gavinsun0921.github.io/tags/dpm/>DPM</a></li><li><a href=https://gavinsun0921.github.io/tags/computer-vision/>Computer Vision</a></li><li><a href=https://gavinsun0921.github.io/tags/image-generation/>Image Generation</a></li><li><a href=https://gavinsun0921.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://gavinsun0921.github.io/posts/paper-research-02/><span class=title>« Prev</span><br><span>A Brief Exploration to Variational Autoencoder (VAE) with Code Implementation</span>
</a><a class=next href=https://gavinsun0921.github.io/posts/fast-paper-reading-02/><span class=title>Next »</span><br><span>[T-PAMI'23] Image Super-Resolution via Iterative Refinement 速读</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读 on x" href="https://x.com/intent/tweet/?text=%5bNeurIPS%2719%20Oral%5d%20Generative%20Modeling%20by%20Estimating%20Gradients%20of%20the%20Data%20Distribution%20%e9%80%9f%e8%af%bb&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-03%2f&amp;hashtags=Diffusion%2cDPM%2cComputerVision%2cImageGeneration%2cDeepLearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-03%2f&amp;title=%5bNeurIPS%2719%20Oral%5d%20Generative%20Modeling%20by%20Estimating%20Gradients%20of%20the%20Data%20Distribution%20%e9%80%9f%e8%af%bb&amp;summary=%5bNeurIPS%2719%20Oral%5d%20Generative%20Modeling%20by%20Estimating%20Gradients%20of%20the%20Data%20Distribution%20%e9%80%9f%e8%af%bb&amp;source=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-03%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-03%2f&title=%5bNeurIPS%2719%20Oral%5d%20Generative%20Modeling%20by%20Estimating%20Gradients%20of%20the%20Data%20Distribution%20%e9%80%9f%e8%af%bb"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-03%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读 on whatsapp" href="https://api.whatsapp.com/send?text=%5bNeurIPS%2719%20Oral%5d%20Generative%20Modeling%20by%20Estimating%20Gradients%20of%20the%20Data%20Distribution%20%e9%80%9f%e8%af%bb%20-%20https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-03%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读 on telegram" href="https://telegram.me/share/url?text=%5bNeurIPS%2719%20Oral%5d%20Generative%20Modeling%20by%20Estimating%20Gradients%20of%20the%20Data%20Distribution%20%e9%80%9f%e8%af%bb&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-03%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [NeurIPS'19 Oral] Generative Modeling by Estimating Gradients of the Data Distribution 速读 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bNeurIPS%2719%20Oral%5d%20Generative%20Modeling%20by%20Estimating%20Gradients%20of%20the%20Data%20Distribution%20%e9%80%9f%e8%af%bb&u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-03%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=GavinSun0921/gavinsun0921.github.io data-repo-id=R_kgDOJgiWSg data-category=Announcements data-category-id=DIC_kwDOJgiWSs4CtrHs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://gavinsun0921.github.io/>Gavin's Home</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>