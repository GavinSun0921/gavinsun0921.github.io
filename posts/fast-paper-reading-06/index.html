<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告 | Gavin's Home</title><meta name=keywords content="Point Tracking,4D Reconstruction,Computer Vision"><meta name=description content="本文提出了一个 feed-forward 框架，通过引入一种创新的、依赖于时间的 pointmap 表示，并利用一个双分支 Transformer 架构，实现了在统一的世界坐标系中同时进行动态场景的密集追踪与三维重建。"><meta name=author content><link rel=canonical href=https://gavinsun0921.github.io/posts/fast-paper-reading-06/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://gavinsun0921.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gavinsun0921.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gavinsun0921.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gavinsun0921.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gavinsun0921.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gavinsun0921.github.io/posts/fast-paper-reading-06/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredTags:["script","noscript","style","textarea","pre"],throwOnError:!1})'></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-18P7N9RZDS"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-18P7N9RZDS")</script><meta property="og:url" content="https://gavinsun0921.github.io/posts/fast-paper-reading-06/"><meta property="og:site_name" content="Gavin's Home"><meta property="og:title" content="[ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告"><meta property="og:description" content="本文提出了一个 feed-forward 框架，通过引入一种创新的、依赖于时间的 pointmap 表示，并利用一个双分支 Transformer 架构，实现了在统一的世界坐标系中同时进行动态场景的密集追踪与三维重建。"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-05T00:00:00+00:00"><meta property="article:tag" content="Point Tracking"><meta property="article:tag" content="4D Reconstruction"><meta property="article:tag" content="Computer Vision"><meta name=twitter:card content="summary"><meta name=twitter:title content="[ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告"><meta name=twitter:description content="本文提出了一个 feed-forward 框架，通过引入一种创新的、依赖于时间的 pointmap 表示，并利用一个双分支 Transformer 架构，实现了在统一的世界坐标系中同时进行动态场景的密集追踪与三维重建。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gavinsun0921.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告","item":"https://gavinsun0921.github.io/posts/fast-paper-reading-06/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告","name":"[ICCV\u002725] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告","description":"本文提出了一个 feed-forward 框架，通过引入一种创新的、依赖于时间的 pointmap 表示，并利用一个双分支 Transformer 架构，实现了在统一的世界坐标系中同时进行动态场景的密集追踪与三维重建。","keywords":["Point Tracking","4D Reconstruction","Computer Vision"],"articleBody":"TL;DR 本文提出了一个 feed-forward 框架，通过引入一种创新的、依赖于时间的 pointmap 表示，并利用一个双分支 Transformer 架构，实现了在统一的世界坐标系中同时进行动态场景的密集追踪与三维重建。\n1. 研究动机 (Motivation) 1.1 背景 (Background) 在计算机视觉中，“对应关系”是三维重建的基石。在静态场景中，三维几何和二维对应是同一枚硬币的两面。\n1.2 现有方法的局限性 (Gap/Limitations of Existing Work) 当场景变为动态时，这种几何与对应的协同关系似乎被打破了。现有方法，特别是数据驱动的方法，往往将动态场景重建和点追踪（即寻找对应关系）视为两个独立且不相关的任务。作者认为，这是一种“错失的机会”，因为动态场景中的这种协同关系并未消失，只是需要额外理解场景内容如何随时间演化，即3D运动估计（3D点追踪）。\n“We argue that this is a missed opportunity; the synergy between 3D reconstruction and 2D correspondence is not lost in dynamic scenes—it simply requires an additional element: understanding how the scene content evolves over time.” (p.1, Section 1)\n1.3 本文价值 (Value Proposition) 本文旨在重新建立动态场景下三维重建与追踪之间的联系。\nSt4RTrack 提出了一个统一的学习框架，能够直接从RGB视频中，在一个一致的世界坐标系里，同时完成动态内容的重建与追踪。这种在世界坐标系中进行追踪的方式，能从根本上解耦场景运动和相机运动。\nFig. 1. St4RTrack\n2. 解决的关键问题与贡献 (Key Problem Solved \u0026 Contribution) 2.1 解决的关键技术问题 如何设计一个统一的 feed-forward 模型，它能够仅通过重新定义其输出表示，就能自然地将动态场景的三维重建任务和三维点追踪任务融合在一起，并直接在统一的世界坐标系中输出结果？\n2.2 核心贡献 统一的 4D 表示: 本文的核心思想源于一个关键的观察：一个静态的 3D 重建方法（DUSt3R）只需改变其 pointmap 的标注方式，就能适应动态场景（MonST3R）。基于此，本文提出了一种新的、依赖于时间的 pointmap 定义，通过预测两张精心定义的 pointmap 来统一重建与追踪任务。 同时重建与追踪的架构: 实现了一个双分支的 Transformer 架构。其中“重建分支”负责重建目标帧的几何，“追踪分支”则负责预测参考帧的几何内容如何运动到目标帧的时刻。 无需4D真值的自适应方案: 提出了一种新颖的 test-time adaptation 方案。通过一个可微的PnP模块来求解相机参数，进而利用2D追踪的伪标签和单目深度先验构成 reprojection loss，使得模型能够从未标注的真实视频中学习，适应新领域。 新的评测基准：针对世界坐标系下的 3D 追踪任务，建立了一个新的评测基准 WorldTrack，以评估和推动相关研究。 3. 方法详述 (Method) St4RTrack 的方法核心在于对 pointmap 概念的重新思考和扩展，并围绕此构建了一个双分支 feed-forward 网络，最终通过 reprojection loss 实现自适应。\n3.1 统一的 4D 表示 (Unified 4D Representation) 这是理解本文方法的关键。作者引入了时间作为 pointmap 的一个决定性因素 。\n时间依赖的 Pointmap 定义：作者提出了一个更泛化的 pointmap 表示: $$^{\\color{red}a}\\mathbf{X}_{\\color{green}t}^{\\color{blue}b}$$ $\\color{blue}b$: pointmap 所描述的物理内容来源是第 b 帧图像。 $\\color{green}t$: pointmap 所描述的是在 t 时刻的场景状态。 $\\color{red}a$: pointmap 的三维坐标是在第 a 帧的相机坐标下表达的。 St4RTrack 的核心 pipeline：如图 3 所示，对于输入的一对图像 $(\\mathbf{I}_1, \\mathbf{I}_j)$，St4RTrack 模型 $f_\\theta$ 会输出两个 pointmap：$$f_\\theta(\\mathbf{I}_1, \\mathbf{I}_j)={^1\\mathbf{X}^1_j, ^1\\mathbf{X}^j_j}$$ $^1\\mathbf{X}^j_j$（重建分支）：这个 pointmap 描述的是第 j 帧的内容，在第 j 帧的时刻，在第 1 帧的坐标系下表达。这本质上就是动态场景重建：将 j 帧的场景重建到 1 帧的坐标系下。 $^1\\mathbf{X}^1_j$（追踪分支）：这个 pointmap 描述的是第 1 帧的内容，在第 j 帧的时刻，在第 1 帧的坐标系下表达。这本质上就是 3D 点追踪：它回答了“第 1 帧的那些点，在第 j 帧的那个时刻，移动到了世界坐标系下的什么位置？” 当处理整个视频时，模型始终将第一帧 $\\mathbf{I}_1$ 作为参考（即世界坐标），依次计算 $f_\\theta(\\mathbf{I}_1, \\mathbf{I}_j)$ 对 $(j=1,2,\\cdots, T)$。这样，输出的 $^1\\mathbf{X}^1_j$ 序列就构成了对第一帧所有点的密集 3D 追踪轨迹，而 $^1\\mathbf{X}^j_j$ 序列则构成了整个视频的动态三维重建。\nFig. 3. Overview of St4RTrack\n3.2 联合学习 (Joint Learning) 网络架构：St4RTrack 采用了一个与 DUSt3R 类似的双分支（siamese）Transformer 架构。两个输入图像 $\\mathbf{I}_1, \\mathbf{I}_j$ 分别通过 ViT Encoder，然后在 Decoder 中通过自注意力和交叉注意力进行信息交互，最终由不同的 Head 输出各自的 pointmap。虽然两个分支共享结构，但它们的目标不同，分别对应“最终”和“重建”。 有监督预训练：由于追踪分支需要知道点在世界坐标系中的真实运动，模型首先在提供完整 4D 信息的合成数据集（如 Point Odyssey, Dynamic Replica）上进行预训练。使用 ground-truth 的相机参数、深度图和顶点轨迹来监督两个分支的 pointmap 输出。 3.3 无 4D 标签的自适应 (Adapt to Any Video) 这是本文的另一个亮点，使得模型能够应用于没有4D真值的真实视频。\n3.3.1 可微的相机参数求解 模型首先像 DUSt3R 一样，从追踪分支的输出 $^1\\mathbf{X}^1_1$ 中估计出相机内参 $\\mathbf{K}$。 然后，利用重建分支的输出 $^1\\mathbf{X}^j_j$。这个输出为第 j 帧的每个像素 $\\mathbf{x}^{j,n}$ 提供了一个在世界坐标系（即第 1 帧坐标系）下的 3D 坐标 $\\mathbf{X}^{j,n}_j$。这就构成了一组 2D-3D 对应点。 利用这些对应，可以通过 PnP 算法求解第 j 帧的相机外参 $\\mathbf{P}^j = [ \\mathbf{R}^j | \\mathbf{T}^j ]$。 为了让损失能够反向传播，作者采用了一个可微的 PnP 求解器（基于 Gauss-Newton）。 3.3.2 Reprojection Loss 一旦获得了可微的相机位姿 $\\mathbf{P}^j$，就可以构建用于自监督优化的 reprojection loss。这个损失由三个部分构成\n$\\mathcal{L}_\\text{traj}$（轨迹损失）：将追踪分支输出的 3D 点 $^1\\mathbf{X}^1_j$ 投影回第 j 帧的图像平面，得到预测的 2D 轨迹点 $\\hat{\\mathbf{x}}^{j,n}$。然后，将其与一个强大的现成 2D 追踪器（如CoTracker3）提供的伪标签 $\\mathbf{x}^{j,n}_\\text{trk}$ 进行比较，计算尺度不变的 L2 损失。 $\\mathcal{L}_\\text{depth}$（深度损失）：将重建分支输出的 3D 点 $^1\\mathbf{X}^j_j$ 变换到第 j 帧的相机坐标系下，得到预测的深度 $z^{j,n}_\\text{proj}$。然后，将其与一个强大的现成单目深度估计模型（如MoGe）提供的伪标签 $z^{j,n}_\\text{mono}$ 进行比较，计算尺度不变的 L2 损失。 $\\mathcal{L}_\\text{align}$（3D自洽损失）：这是一个 3D 空间中的一致性约束。它要求对于第 1 帧中那些在第 j 帧依然可见的点，其在追踪分支中的 3D 位置 $^1\\mathbf{X}^{1,n}_j$，应该与其对应点在重建分支中的 3D 位置 $^1\\mathbf{X}^{j,n'}_j$ 尽可能接近。这确保了两个分支在同一时刻对同一物理点的预测是一致的。 通过最小化总的 reprojection loss，模型可以在测试时对新的、无标签的视频进行 fine-tuning（test-time adaptation），从而弥补合成数据与真实世界之间的领域鸿沟。在自适应时，作者选择冻结重建分支，以保留从预训练中学到的视图对齐能力。\n4. 实验分析 (Experiments) 4.1 3D Tracking in World Coordinates Tab. 1. World Coordinate 3D Point Tracking\n实验结果显示，St4RTrack 在新提出的 WorldTrack 基准上取得了全面的SOTA性能 。值得注意的是，它显著优于那些复杂的组合基线，证明了其统一建模的优越性。即使在没有相机运动的 Panoptic Studio 数据集上，它的表现也优于专门的相机空间追踪器 SpatialTracker。\n4.2 Dynamic 3D Reconstruction Tab. 2. World Coordinate 3D Reconstruction\n在重建任务上，St4RTrack 同样达到了SOTA水平 。它甚至超过了那些使用了额外全局对齐（Global Alignment）步骤的 MonST3R 等方法。这进一步凸显了其联合进行追踪与重建所带来的好处。\n4.3 Ablation Study Fig. 5. Ablation Study\n预训练的必要性: 图 5 的定性比较清晰地显示，如果没有在合成数据集上进行预训练来学习本文提出的4D表示，即使进行 test-time adaptation，模型的追踪和重建两个分支的输出也无法对齐，效果很差。 Test-Time Adaptation (TTA)的有效性: 图 5 同样证明，TTA能够有效修正模型在真实数据上的漂移问题，使追踪和重建结果更精确。表 6 的结果也显示，TTA带来了显著的性能提升。 Reprojection Loss 各部分的贡献：表 6 的最后三行显示，在TTA中去掉轨迹损失、深度损失或3D自洽损失中的任何一项，都会导致性能下降，证明了这三个损失分量对于模型的自适应都至关重要。 Tab. 6. World Coordinate 3D Tracking (Median-Scale) （这个图在原论文的补充材料中）\n5. 批判性思考 (Critical Analysis \u0026 Personal Thoughts) 5.1 优点 (Strengths) 概念的优雅与统一: 本文最大的亮点在于其思想的统一性。通过对 pointmap 表示进行巧妙的重新定义，将追踪和重建这两个看似独立的任务，内生地、优雅地统一到了一个框架下。这种“表示即方案”的思路极具启发性。 直击问题本质: 它直接在世界坐标系中进行操作，从根本上解决了相机运动和物体运动的纠缠问题，而不是像其他方法那样进行“解耦”或“分离”。这是一种更直接、更符合第一性原理的解决方案。 创新的自适应机制: Test-time adaptation 的设计非常巧妙。它通过可微的 PnP 求解器和利用现成模型作为伪标签的 reprojection loss，为如何让一个在合成数据上训练的复杂 4D 模型成功应用于无标签的真实世界视频，提供了一个非常有效的范本。 5.2 潜在缺点/疑问 (Weaknesses/Questions) 对锚定帧的依赖: 整个框架将第一帧作为世界坐标系的绝对参考。这意味着如果视频的第一帧质量不佳（例如，模糊、遮挡严重），可能会影响后续所有帧的重建和追踪精度。整个系统的“地基”完全由第一帧决定。 长视频的可扩展性: St4RTrack 采用的是将每一帧都与第一帧配对的策略。对于非常长的视频，这种方法可能会忽略相邻帧之间丰富的时序信息。作者在讨论部分也承认了这是一个局限，并提出未来可以引入跨多帧的 temporal attention 来缓解。 Test-Time Adaptation 的成本: 虽然 TTA 效果显著，但它需要在测试时对每个视频序列进行额外的优化（在4块A100上约需5分钟）。这相对于纯粹的 feed-forward 推理（30 FPS），在需要即时响应的应用中是一个不可忽视的成本。 5.3 启发/可借鉴点 (Insights/Takeaways) 表示的力量: 这篇论文再次证明，一个好的数据表示（Representation）本身就是一种解决方案。通过引入时间维度，作者将一个复杂的多任务问题转化为了一个统一的表示预测问题。 利用先验进行自监督: “利用现成的、强大的模型（如CoTracker3, MoGe）的输出作为伪标签来构建损失函数”是一种非常实用的策略。这使得模型可以在没有昂贵真值标注的情况下，从海量真实数据中学习。 Sim-to-Real的有效路径: “在合成数据上预训练以学习核心概念和表示” + “在真实数据上通过自监督损失进行微调/自适应”，是解决模拟到现实（Sim-to-Real）领域鸿沟的一条黄金路径。 ","wordCount":"478","inLanguage":"en","datePublished":"2025-08-05T00:00:00Z","dateModified":"2025-08-05T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://gavinsun0921.github.io/posts/fast-paper-reading-06/"},"publisher":{"@type":"Organization","name":"Gavin's Home","logo":{"@type":"ImageObject","url":"https://gavinsun0921.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gavinsun0921.github.io/ accesskey=h title="Gavin's Home (Alt + H)">Gavin's Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gavinsun0921.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://gavinsun0921.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gavinsun0921.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://gavinsun0921.github.io/about/ title=AboutMe><span>AboutMe</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告</h1><div class=post-meta><span title='2025-08-05 00:00:00 +0000 UTC'>August 5, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;478 words</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tldr aria-label=TL;DR>TL;DR</a></li><li><a href=#1-%e7%a0%94%e7%a9%b6%e5%8a%a8%e6%9c%ba-motivation aria-label="1. 研究动机 (Motivation)">1. 研究动机 (Motivation)</a><ul><li><a href=#11-%e8%83%8c%e6%99%af-background aria-label="1.1 背景 (Background)">1.1 背景 (Background)</a></li><li><a href=#12-%e7%8e%b0%e6%9c%89%e6%96%b9%e6%b3%95%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7-gaplimitations-of-existing-work aria-label="1.2 现有方法的局限性 (Gap/Limitations of Existing Work)">1.2 现有方法的局限性 (Gap/Limitations of Existing Work)</a></li><li><a href=#13-%e6%9c%ac%e6%96%87%e4%bb%b7%e5%80%bc-value-proposition aria-label="1.3 本文价值 (Value Proposition)">1.3 本文价值 (Value Proposition)</a></li></ul></li><li><a href=#2-%e8%a7%a3%e5%86%b3%e7%9a%84%e5%85%b3%e9%94%ae%e9%97%ae%e9%a2%98%e4%b8%8e%e8%b4%a1%e7%8c%ae-key-problem-solved--contribution aria-label="2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)">2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)</a><ul><li><a href=#21-%e8%a7%a3%e5%86%b3%e7%9a%84%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e9%97%ae%e9%a2%98 aria-label="2.1 解决的关键技术问题">2.1 解决的关键技术问题</a></li><li><a href=#22-%e6%a0%b8%e5%bf%83%e8%b4%a1%e7%8c%ae aria-label="2.2 核心贡献">2.2 核心贡献</a></li></ul></li><li><a href=#3-%e6%96%b9%e6%b3%95%e8%af%a6%e8%bf%b0-method aria-label="3. 方法详述 (Method)">3. 方法详述 (Method)</a><ul><li><a href=#31-%e7%bb%9f%e4%b8%80%e7%9a%84-4d-%e8%a1%a8%e7%a4%ba-unified-4d-representation aria-label="3.1 统一的 4D 表示 (Unified 4D Representation)">3.1 统一的 4D 表示 (Unified 4D Representation)</a></li><li><a href=#32-%e8%81%94%e5%90%88%e5%ad%a6%e4%b9%a0-joint-learning aria-label="3.2 联合学习 (Joint Learning)">3.2 联合学习 (Joint Learning)</a></li><li><a href=#33-%e6%97%a0-4d-%e6%a0%87%e7%ad%be%e7%9a%84%e8%87%aa%e9%80%82%e5%ba%94-adapt-to-any-video aria-label="3.3 无 4D 标签的自适应 (Adapt to Any Video)">3.3 无 4D 标签的自适应 (Adapt to Any Video)</a><ul><li><a href=#331-%e5%8f%af%e5%be%ae%e7%9a%84%e7%9b%b8%e6%9c%ba%e5%8f%82%e6%95%b0%e6%b1%82%e8%a7%a3 aria-label="3.3.1 可微的相机参数求解">3.3.1 可微的相机参数求解</a></li><li><a href=#332-reprojection-loss aria-label="3.3.2 Reprojection Loss">3.3.2 Reprojection Loss</a></li></ul></li></ul></li><li><a href=#4-%e5%ae%9e%e9%aa%8c%e5%88%86%e6%9e%90-experiments aria-label="4. 实验分析 (Experiments)">4. 实验分析 (Experiments)</a><ul><li><a href=#41-3d-tracking-in-world-coordinates aria-label="4.1 3D Tracking in World Coordinates">4.1 3D Tracking in World Coordinates</a></li><li><a href=#42-dynamic-3d-reconstruction aria-label="4.2 Dynamic 3D Reconstruction">4.2 Dynamic 3D Reconstruction</a></li><li><a href=#43-ablation-study aria-label="4.3 Ablation Study">4.3 Ablation Study</a></li></ul></li><li><a href=#5-%e6%89%b9%e5%88%a4%e6%80%a7%e6%80%9d%e8%80%83-critical-analysis--personal-thoughts aria-label="5. 批判性思考 (Critical Analysis & Personal Thoughts)">5. 批判性思考 (Critical Analysis & Personal Thoughts)</a><ul><li><a href=#51-%e4%bc%98%e7%82%b9-strengths aria-label="5.1 优点 (Strengths)">5.1 优点 (Strengths)</a></li><li><a href=#52-%e6%bd%9c%e5%9c%a8%e7%bc%ba%e7%82%b9%e7%96%91%e9%97%ae-weaknessesquestions aria-label="5.2 潜在缺点/疑问 (Weaknesses/Questions)">5.2 潜在缺点/疑问 (Weaknesses/Questions)</a></li><li><a href=#53-%e5%90%af%e5%8f%91%e5%8f%af%e5%80%9f%e9%89%b4%e7%82%b9-insightstakeaways aria-label="5.3 启发/可借鉴点 (Insights/Takeaways)">5.3 启发/可借鉴点 (Insights/Takeaways)</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=tldr>TL;DR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>本文提出了一个 feed-forward 框架，通过引入一种创新的、依赖于时间的 pointmap 表示，并利用一个双分支 Transformer 架构，实现了在统一的世界坐标系中同时进行动态场景的密集追踪与三维重建。</p><h2 id=1-研究动机-motivation>1. 研究动机 (Motivation)<a hidden class=anchor aria-hidden=true href=#1-研究动机-motivation>#</a></h2><h3 id=11-背景-background>1.1 背景 (Background)<a hidden class=anchor aria-hidden=true href=#11-背景-background>#</a></h3><p>在计算机视觉中，“对应关系”是三维重建的基石。在静态场景中，三维几何和二维对应是同一枚硬币的两面。</p><h3 id=12-现有方法的局限性-gaplimitations-of-existing-work>1.2 现有方法的局限性 (Gap/Limitations of Existing Work)<a hidden class=anchor aria-hidden=true href=#12-现有方法的局限性-gaplimitations-of-existing-work>#</a></h3><p>当场景变为动态时，这种几何与对应的协同关系似乎被打破了。现有方法，特别是数据驱动的方法，往往将<strong>动态场景重建</strong>和<strong>点追踪</strong>（即寻找对应关系）视为两个独立且不相关的任务。作者认为，这是一种“错失的机会”，因为动态场景中的这种协同关系并未消失，只是需要额外理解场景内容如何随时间演化，即3D运动估计（3D点追踪）。</p><blockquote><p>&ldquo;We argue that this is a <strong>missed opportunity</strong>; the synergy between 3D reconstruction and 2D correspondence is not lost in dynamic scenes—it simply requires an additional element: understanding how the scene content evolves over time.&rdquo; (p.1, Section 1)</p></blockquote><h3 id=13-本文价值-value-proposition>1.3 本文价值 (Value Proposition)<a hidden class=anchor aria-hidden=true href=#13-本文价值-value-proposition>#</a></h3><p>本文旨在重新建立动态场景下三维重建与追踪之间的联系。</p><p>St4RTrack 提出了一个统一的学习框架，能够直接从RGB视频中，在一个一致的世界坐标系里，<strong>同时</strong>完成动态内容的重建与追踪。这种在世界坐标系中进行追踪的方式，能从根本上解耦场景运动和相机运动。</p><figure class=align-center><img loading=lazy src=images/St4RTrack.jpg#center width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 1. St4RTrack</p></figcaption></figure><h2 id=2-解决的关键问题与贡献-key-problem-solved--contribution>2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)<a hidden class=anchor aria-hidden=true href=#2-解决的关键问题与贡献-key-problem-solved--contribution>#</a></h2><h3 id=21-解决的关键技术问题>2.1 解决的关键技术问题<a hidden class=anchor aria-hidden=true href=#21-解决的关键技术问题>#</a></h3><p>如何设计一个统一的 feed-forward 模型，它能够仅通过重新定义其输出表示，就能自然地将动态场景的三维重建任务和三维点追踪任务融合在一起，并直接在统一的世界坐标系中输出结果？</p><h3 id=22-核心贡献>2.2 核心贡献<a hidden class=anchor aria-hidden=true href=#22-核心贡献>#</a></h3><ol><li><strong>统一的 4D 表示</strong>: 本文的核心思想源于一个关键的观察：一个静态的 3D 重建方法（DUSt3R）只需改变其 pointmap 的标注方式，就能适应动态场景（MonST3R）。基于此，本文提出了一种新的、依赖于时间的 pointmap 定义，通过预测两张精心定义的 pointmap 来统一重建与追踪任务。</li><li><strong>同时重建与追踪的架构</strong>: 实现了一个双分支的 Transformer 架构。其中“重建分支”负责重建目标帧的几何，“追踪分支”则负责预测参考帧的几何内容如何运动到目标帧的时刻。</li><li><strong>无需4D真值的自适应方案</strong>: 提出了一种新颖的 test-time adaptation 方案。通过一个可微的PnP模块来求解相机参数，进而利用2D追踪的伪标签和单目深度先验构成 reprojection loss，使得模型能够从未标注的真实视频中学习，适应新领域。</li><li><strong>新的评测基准</strong>：针对世界坐标系下的 3D 追踪任务，建立了一个新的评测基准 WorldTrack，以评估和推动相关研究。</li></ol><h2 id=3-方法详述-method>3. 方法详述 (Method)<a hidden class=anchor aria-hidden=true href=#3-方法详述-method>#</a></h2><p>St4RTrack 的方法核心在于对 pointmap 概念的重新思考和扩展，并围绕此构建了一个双分支 feed-forward 网络，最终通过 reprojection loss 实现自适应。</p><h3 id=31-统一的-4d-表示-unified-4d-representation>3.1 统一的 4D 表示 (Unified 4D Representation)<a hidden class=anchor aria-hidden=true href=#31-统一的-4d-表示-unified-4d-representation>#</a></h3><p>这是理解本文方法的关键。作者引入了<strong>时间</strong>作为 pointmap 的一个决定性因素 。</p><ul><li>时间依赖的 Pointmap 定义：作者提出了一个更泛化的 pointmap 表示: $$^{\color{red}a}\mathbf{X}_{\color{green}t}^{\color{blue}b}$$<ul><li>$\color{blue}b$: pointmap 所描述的物理内容来源是第 b 帧图像。</li><li>$\color{green}t$: pointmap 所描述的是在 t 时刻的场景状态。</li><li>$\color{red}a$: pointmap 的三维坐标是在第 a 帧的相机坐标下表达的。</li></ul></li><li>St4RTrack 的核心 pipeline：如图 3 所示，对于输入的一对图像 $(\mathbf{I}_1, \mathbf{I}_j)$，St4RTrack 模型 $f_\theta$ 会输出两个 pointmap：<div>$$f_\theta(\mathbf{I}_1, \mathbf{I}_j)={^1\mathbf{X}^1_j, ^1\mathbf{X}^j_j}$$</div><ul><li>$^1\mathbf{X}^j_j$（重建分支）：这个 pointmap 描述的是第 j 帧的内容，在第 j 帧的时刻，在第 1 帧的坐标系下表达。这本质上就是<strong>动态场景重建</strong>：将 j 帧的场景重建到 1 帧的坐标系下。</li><li>$^1\mathbf{X}^1_j$（追踪分支）：这个 pointmap 描述的是第 1 帧的内容，在第 j 帧的时刻，在第 1 帧的坐标系下表达。这本质上就是 <strong>3D 点追踪</strong>：它回答了“第 1 帧的那些点，在第 j 帧的那个时刻，移动到了世界坐标系下的什么位置？”</li></ul></li></ul><p>当处理整个视频时，模型始终将第一帧 $\mathbf{I}_1$ 作为参考（即世界坐标），依次计算 $f_\theta(\mathbf{I}_1, \mathbf{I}_j)$ 对 $(j=1,2,\cdots, T)$。这样，输出的 $^1\mathbf{X}^1_j$ 序列就构成了对第一帧所有点的密集 3D 追踪轨迹，而 $^1\mathbf{X}^j_j$ 序列则构成了整个视频的动态三维重建。</p><figure class=align-center><img loading=lazy src=images/Overview.jpg#center width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 3. Overview of St4RTrack</p></figcaption></figure><h3 id=32-联合学习-joint-learning>3.2 联合学习 (Joint Learning)<a hidden class=anchor aria-hidden=true href=#32-联合学习-joint-learning>#</a></h3><ul><li><strong>网络架构</strong>：St4RTrack 采用了一个与 DUSt3R 类似的双分支（siamese）Transformer 架构。两个输入图像 $\mathbf{I}_1, \mathbf{I}_j$ 分别通过 ViT Encoder，然后在 Decoder 中通过自注意力和交叉注意力进行信息交互，最终由不同的 Head 输出各自的 pointmap。虽然两个分支共享结构，但它们的目标不同，分别对应“最终”和“重建”。</li><li><strong>有监督预训练</strong>：由于追踪分支需要知道点在世界坐标系中的真实运动，模型首先在提供完整 4D 信息的合成数据集（如 Point Odyssey, Dynamic Replica）上进行预训练。使用 ground-truth 的相机参数、深度图和顶点轨迹来监督两个分支的 pointmap 输出。</li></ul><h3 id=33-无-4d-标签的自适应-adapt-to-any-video>3.3 无 4D 标签的自适应 (Adapt to Any Video)<a hidden class=anchor aria-hidden=true href=#33-无-4d-标签的自适应-adapt-to-any-video>#</a></h3><p>这是本文的另一个亮点，使得模型能够应用于没有4D真值的真实视频。</p><h4 id=331-可微的相机参数求解>3.3.1 可微的相机参数求解<a hidden class=anchor aria-hidden=true href=#331-可微的相机参数求解>#</a></h4><ul><li>模型首先像 DUSt3R 一样，从追踪分支的输出 $^1\mathbf{X}^1_1$ 中估计出相机内参 $\mathbf{K}$。</li><li>然后，利用重建分支的输出 $^1\mathbf{X}^j_j$。这个输出为第 j 帧的每个像素 $\mathbf{x}^{j,n}$ 提供了一个在世界坐标系（即第 1 帧坐标系）下的 3D 坐标 $\mathbf{X}^{j,n}_j$。这就构成了一组 2D-3D 对应点。</li><li>利用这些对应，可以通过 PnP 算法求解第 j 帧的相机外参 $\mathbf{P}^j = [ \mathbf{R}^j | \mathbf{T}^j ]$。</li><li>为了让损失能够反向传播，作者采用了一个可微的 PnP 求解器（基于 Gauss-Newton）。</li></ul><h4 id=332-reprojection-loss>3.3.2 Reprojection Loss<a hidden class=anchor aria-hidden=true href=#332-reprojection-loss>#</a></h4><p>一旦获得了可微的相机位姿 $\mathbf{P}^j$，就可以构建用于自监督优化的 reprojection loss。这个损失由三个部分构成</p><ul><li>$\mathcal{L}_\text{traj}$（<strong>轨迹损失</strong>）：将追踪分支输出的 3D 点 $^1\mathbf{X}^1_j$ 投影回第 j 帧的图像平面，得到预测的 2D 轨迹点 $\hat{\mathbf{x}}^{j,n}$。然后，将其与一个强大的现成 2D 追踪器（如CoTracker3）提供的伪标签 $\mathbf{x}^{j,n}_\text{trk}$ 进行比较，计算尺度不变的 L2 损失。</li><li>$\mathcal{L}_\text{depth}$（<strong>深度损失</strong>）：将重建分支输出的 3D 点 $^1\mathbf{X}^j_j$ 变换到第 j 帧的相机坐标系下，得到预测的深度 $z^{j,n}_\text{proj}$。然后，将其与一个强大的现成单目深度估计模型（如MoGe）提供的伪标签 $z^{j,n}_\text{mono}$ 进行比较，计算尺度不变的 L2 损失。</li><li>$\mathcal{L}_\text{align}$（<strong>3D自洽损失</strong>）：这是一个 3D 空间中的一致性约束。它要求对于第 1 帧中那些在第 j 帧依然可见的点，其在追踪分支中的 3D 位置 $^1\mathbf{X}^{1,n}_j$，应该与其对应点在重建分支中的 3D 位置 $^1\mathbf{X}^{j,n'}_j$ 尽可能接近。这确保了两个分支在同一时刻对同一物理点的预测是一致的。</li></ul><p>通过最小化总的 reprojection loss，模型可以在测试时对新的、无标签的视频进行 fine-tuning（test-time adaptation），从而弥补合成数据与真实世界之间的领域鸿沟。在自适应时，作者选择冻结重建分支，以保留从预训练中学到的视图对齐能力。</p><h2 id=4-实验分析-experiments>4. 实验分析 (Experiments)<a hidden class=anchor aria-hidden=true href=#4-实验分析-experiments>#</a></h2><h3 id=41-3d-tracking-in-world-coordinates>4.1 3D Tracking in World Coordinates<a hidden class=anchor aria-hidden=true href=#41-3d-tracking-in-world-coordinates>#</a></h3><figure class=align-center><img loading=lazy src=images/tab1.jpg#center width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 1. World Coordinate 3D Point Tracking</p></figcaption></figure><p>实验结果显示，St4RTrack 在新提出的 WorldTrack 基准上取得了全面的SOTA性能 。值得注意的是，它显著优于那些复杂的组合基线，证明了其统一建模的优越性。即使在没有相机运动的 Panoptic Studio 数据集上，它的表现也优于专门的相机空间追踪器 SpatialTracker。</p><h3 id=42-dynamic-3d-reconstruction>4.2 Dynamic 3D Reconstruction<a hidden class=anchor aria-hidden=true href=#42-dynamic-3d-reconstruction>#</a></h3><figure class=align-center><img loading=lazy src=images/tab2.jpg#center width=55% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 2. World Coordinate 3D Reconstruction</p></figcaption></figure><p>在重建任务上，St4RTrack 同样达到了SOTA水平 。它甚至超过了那些使用了额外全局对齐（Global Alignment）步骤的 MonST3R 等方法。这进一步凸显了其联合进行追踪与重建所带来的好处。</p><h3 id=43-ablation-study>4.3 Ablation Study<a hidden class=anchor aria-hidden=true href=#43-ablation-study>#</a></h3><figure class=align-center><img loading=lazy src=images/ablation.jpg#center width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 5. Ablation Study</p></figcaption></figure><ul><li><strong>预训练的必要性</strong>: 图 5 的定性比较清晰地显示，如果没有在合成数据集上进行预训练来学习本文提出的4D表示，即使进行 test-time adaptation，模型的追踪和重建两个分支的输出也无法对齐，效果很差。</li><li><strong>Test-Time Adaptation (TTA)的有效性</strong>: 图 5 同样证明，TTA能够有效修正模型在真实数据上的漂移问题，使追踪和重建结果更精确。表 6 的结果也显示，TTA带来了显著的性能提升。</li><li><strong>Reprojection Loss 各部分的贡献</strong>：表 6 的最后三行显示，在TTA中去掉轨迹损失、深度损失或3D自洽损失中的任何一项，都会导致性能下降，证明了这三个损失分量对于模型的自适应都至关重要。</li></ul><figure class=align-center><img loading=lazy src=images/tab2.jpg#center width=55% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 6. World Coordinate 3D Tracking (Median-Scale) （这个图在原论文的补充材料中）</p></figcaption></figure><h2 id=5-批判性思考-critical-analysis--personal-thoughts>5. 批判性思考 (Critical Analysis & Personal Thoughts)<a hidden class=anchor aria-hidden=true href=#5-批判性思考-critical-analysis--personal-thoughts>#</a></h2><h3 id=51-优点-strengths>5.1 优点 (Strengths)<a hidden class=anchor aria-hidden=true href=#51-优点-strengths>#</a></h3><ul><li><strong>概念的优雅与统一</strong>: 本文最大的亮点在于其思想的统一性。通过对 pointmap 表示进行巧妙的重新定义，将追踪和重建这两个看似独立的任务，内生地、优雅地统一到了一个框架下。这种“表示即方案”的思路极具启发性。</li><li><strong>直击问题本质</strong>: 它直接在世界坐标系中进行操作，从根本上解决了相机运动和物体运动的纠缠问题，而不是像其他方法那样进行“解耦”或“分离”。这是一种更直接、更符合第一性原理的解决方案。</li><li><strong>创新的自适应机制</strong>: Test-time adaptation 的设计非常巧妙。它通过可微的 PnP 求解器和利用现成模型作为伪标签的 reprojection loss，为如何让一个在合成数据上训练的复杂 4D 模型成功应用于无标签的真实世界视频，提供了一个非常有效的范本。</li></ul><h3 id=52-潜在缺点疑问-weaknessesquestions>5.2 潜在缺点/疑问 (Weaknesses/Questions)<a hidden class=anchor aria-hidden=true href=#52-潜在缺点疑问-weaknessesquestions>#</a></h3><ul><li><strong>对锚定帧的依赖</strong>: 整个框架将第一帧作为世界坐标系的绝对参考。这意味着如果视频的第一帧质量不佳（例如，模糊、遮挡严重），可能会影响后续所有帧的重建和追踪精度。整个系统的“地基”完全由第一帧决定。</li><li><strong>长视频的可扩展性</strong>: St4RTrack 采用的是将每一帧都与第一帧配对的策略。对于非常长的视频，这种方法可能会忽略相邻帧之间丰富的时序信息。作者在讨论部分也承认了这是一个局限，并提出未来可以引入跨多帧的 temporal attention 来缓解。</li><li><strong>Test-Time Adaptation 的成本</strong>: 虽然 TTA 效果显著，但它需要在测试时对每个视频序列进行额外的优化（在4块A100上约需5分钟）。这相对于纯粹的 feed-forward 推理（30 FPS），在需要即时响应的应用中是一个不可忽视的成本。</li></ul><h3 id=53-启发可借鉴点-insightstakeaways>5.3 启发/可借鉴点 (Insights/Takeaways)<a hidden class=anchor aria-hidden=true href=#53-启发可借鉴点-insightstakeaways>#</a></h3><ul><li><strong>表示的力量</strong>: 这篇论文再次证明，一个好的数据表示（Representation）本身就是一种解决方案。通过引入时间维度，作者将一个复杂的多任务问题转化为了一个统一的表示预测问题。</li><li><strong>利用先验进行自监督</strong>: “利用现成的、强大的模型（如CoTracker3, MoGe）的输出作为伪标签来构建损失函数”是一种非常实用的策略。这使得模型可以在没有昂贵真值标注的情况下，从海量真实数据中学习。</li><li><strong>Sim-to-Real的有效路径</strong>: “在合成数据上预训练以学习核心概念和表示” + “在真实数据上通过自监督损失进行微调/自适应”，是解决模拟到现实（Sim-to-Real）领域鸿沟的一条黄金路径。</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://gavinsun0921.github.io/tags/point-tracking/>Point Tracking</a></li><li><a href=https://gavinsun0921.github.io/tags/4d-reconstruction/>4D Reconstruction</a></li><li><a href=https://gavinsun0921.github.io/tags/computer-vision/>Computer Vision</a></li></ul><nav class=paginav><a class=next href=https://gavinsun0921.github.io/posts/fast-paper-reading-05/><span class=title>Next »</span><br><span>[ICCV'25 Oral] Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction 速读</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告 on x" href="https://x.com/intent/tweet/?text=%5bICCV%2725%5d%20St4RTrack%3a%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-06%2f&amp;hashtags=PointTracking%2c4DReconstruction%2cComputerVision"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-06%2f&amp;title=%5bICCV%2725%5d%20St4RTrack%3a%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;summary=%5bICCV%2725%5d%20St4RTrack%3a%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;source=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-06%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-06%2f&title=%5bICCV%2725%5d%20St4RTrack%3a%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-06%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告 on whatsapp" href="https://api.whatsapp.com/send?text=%5bICCV%2725%5d%20St4RTrack%3a%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a%20-%20https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-06%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告 on telegram" href="https://telegram.me/share/url?text=%5bICCV%2725%5d%20St4RTrack%3a%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-06%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World 阅读报告 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bICCV%2725%5d%20St4RTrack%3a%20Simultaneous%204D%20Reconstruction%20and%20Tracking%20in%20the%20World%20%e9%98%85%e8%af%bb%e6%8a%a5%e5%91%8a&u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-06%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=GavinSun0921/gavinsun0921.github.io data-repo-id=R_kgDOJgiWSg data-category=Announcements data-category-id=DIC_kwDOJgiWSs4CtrHs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://gavinsun0921.github.io/>Gavin's Home</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>