<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy | Gavin's Home</title><meta name=keywords content="Point Tracking,4D Reconstruction,Computer Vision"><meta name=description content="本文提出了一个 feed-forward 3D point tracking architecture，它将 video depth、camera pose 和 object motion 进行统一建模和 end-to-end 优化，并通过在 17 个异构数据集上的可扩展训练，实现了 SOTA 的 3D 追踪精度和推理速度。"><meta name=author content><link rel=canonical href=https://gavinsun0921.github.io/posts/fast-paper-reading-04/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://gavinsun0921.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gavinsun0921.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gavinsun0921.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gavinsun0921.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gavinsun0921.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gavinsun0921.github.io/posts/fast-paper-reading-04/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],ignoredTags:["script","noscript","style","textarea","pre"],throwOnError:!1})'></script><meta property="og:url" content="https://gavinsun0921.github.io/posts/fast-paper-reading-04/"><meta property="og:site_name" content="Gavin's Home"><meta property="og:title" content="[ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy"><meta property="og:description" content="本文提出了一个 feed-forward 3D point tracking architecture，它将 video depth、camera pose 和 object motion 进行统一建模和 end-to-end 优化，并通过在 17 个异构数据集上的可扩展训练，实现了 SOTA 的 3D 追踪精度和推理速度。"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-29T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-29T00:00:00+00:00"><meta property="article:tag" content="Point Tracking"><meta property="article:tag" content="4D Reconstruction"><meta property="article:tag" content="Computer Vision"><meta name=twitter:card content="summary"><meta name=twitter:title content="[ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy"><meta name=twitter:description content="本文提出了一个 feed-forward 3D point tracking architecture，它将 video depth、camera pose 和 object motion 进行统一建模和 end-to-end 优化，并通过在 17 个异构数据集上的可扩展训练，实现了 SOTA 的 3D 追踪精度和推理速度。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gavinsun0921.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy","item":"https://gavinsun0921.github.io/posts/fast-paper-reading-04/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy","name":"[ICCV\u002725] SpatialTrackerV2: 3D Point Tracking Made Easy","description":"本文提出了一个 feed-forward 3D point tracking architecture，它将 video depth、camera pose 和 object motion 进行统一建模和 end-to-end 优化，并通过在 17 个异构数据集上的可扩展训练，实现了 SOTA 的 3D 追踪精度和推理速度。","keywords":["Point Tracking","4D Reconstruction","Computer Vision"],"articleBody":"TL;DR 本文提出了一个 feed-forward 3D point tracking architecture，它将 video depth、camera pose 和 object motion 进行统一建模和 end-to-end 优化，并通过在 17 个异构数据集上的可扩展训练，实现了 SOTA 的 3D 追踪精度和推理速度。\n1. 研究动机 (Motivation) 1.1 背景 (Background) 3D point tracking，即从弹幕视频中恢复长期的 3D trajectories，是一种通用的 dynamic scene representation，它在机器人、视频生成、3D/4D 重建等领域有巨大潜力 。\n1.2 现有方法的局限性 (Gap/Limitation of Existing Work) 作者指出了当前方法的两大核心痛点：\n模块化 pipeline 导致的误差累积：现有方法大多依赖于现成的视觉模型（如 optical flow 、monocular depth estimation）构建模块化的 pipeline 。这种分离式的处理方式忽略了 scene geometry、camera motion 和 object motion 三者之间内在的强关联性，导致误差在不同模块间传递和累积。 训练数据限制了泛化能力：以往的 feed-forward 3D tracking models 严重依赖带有 ground-truth 3D tracks 的数据集进行监督训练。这类数据集难以大规模获取，导致模型在多样的 in-the-wild 视频上表现不佳，扩展性差。而基于优化的方法虽然效果好，但因其 per-scene optimization 的设计，推理速度很慢。 1.3 本文价值 (Value Proposition) 本文认为必须将 scene geometry、camera motion 和 object motion 三者进行联合推理和显式解耦，并设计一个能利用多样化、弱监督数据源的框架。其价值在于，通过一个统一、可微的 end-to-end pipeline，实现一个高精度、高速度、高泛化性的通用 3D point tracker。\n2. 解决的关键问题与贡献 (Key Problem Solved \u0026 Contribution) 2.1 解决的关键技术问题 如何设计一个可扩展的、feed-forward 的 3D tracking model，该模型能够显式地解耦（disentangle）并联合优化 scene geometry (depth), camera ego-motion (pose) 和 object motion，从而摆脱对 ground-truth 监督的强依赖，并利用海量异构视频数据提升模型的泛化性和鲁棒性？\n2.2 核心贡献 Unified Optimization Framework：提出了一个将 video depth, camera pose 和 pixel-wise 3D motion 分解并集成到一个 fully differentiable, end-to-end pipeline 中的新架构。 SyncFormer 模块：设计了一个名为 SyncFormer 的核心模块，它采用双分支（2D \u0026 3D）结构，通过 cross-attention 进行信息交互，有效解耦了在图像空间（2D）和相机坐标空间（3D）中进行的 trajectories 更新，同时支持在循环中进行可微的 Bundle Adjustment。 Scalable Heterogeneous Training：该框架使得在17个不同类型的数据集上进行大规模联合训练成为可能，这些数据集的监督形式各异（如有标注的RGB-D视频、仅有位姿的视频、甚至是无标签的视频）。 SOTA的性能：实验证明，该方法在 3D tracking benchmark (TAPVid-3D) 上性能相对现有方法提升超过 30%，在 dynamic reconstruction 任务上，其性能与顶尖的 optimization-based 方法相当，而推理速度快50倍。 3. 方法详述 (Method) Fig. 1. SpatialTrackerV2 Pipeline Overview\nSpatialTrackerV2采用了一个前后端架构的设计。\n3.1 Front-end：尺度对齐的 video depth \u0026 camera pose estimation 使用 Temporal Encoder 来预测 consistent video depth，同时一个 Neural Camera Tracker 得到 coarse camera（包括 pose, scale, shift）。\n$$\\mathcal{P}^{t},a,b = \\mathcal{H}(\\mathbf{P}_{tok}, \\mathbf{S}\\_{tok}) \\tag{1}$$ 3.2 Back-end: Joint Motion Optimization Fig. 2. SyncFormer\n核心组件：SyncFormer，一个迭代式的 Transformer module。用来联合优化估计 2D trajectories $\\mathcal{T}^{2d} \\in \\mathbb{R}^{T \\times N \\times 2}$ in UV space 以及 3D trajectories $\\mathcal{T}^{3d} \\in \\mathbb{R}^{T \\times N \\times 3}$ in the camera coordinate system。同时对每一个 trajectory 它还动态估计 visibility probability $p^{vis}$ 和 dynamic probability $p^{dyn}$。\n$$ \\mathcal{T}^{2d}_{k+1}, \\mathcal{T}^{3d}_{k+1}, p^{vis}_{k+1}, p^{vis}_{k+1} = f_{sync}(\\mathcal{T}^{2d}_{k}, \\mathcal{T}^{3d}_{k}, p^{vis}_{k}, p^{vis}_{k}, \\mathcal{P}_k) $$ 在每次迭代中，SyncFormer同时更新 2D trajectories、3D trajectories 和 camera pose。\ncamera pose 通过一个可微的 Bundle Adjustment 过程进行优化，该过程利用了 2D 和 3D 轨迹之间的重投影一致性约束。\nSyncFormer 关键采用了双分支（2D \u0026 3D）解耦设计。2D 和 3D 的 Embeddings 在各自的分之内通过 self attention 处理，并通过 proxy tokens 之间的 cross attention 进行信息交换。这防止了两种不同空间（图像空间 vs. 相机空间）的更新信号相互干扰。\n4. 实验分析 (Experiments) 4.1 3D Point Tracking Tab. 1. 3D Point Tracking Results\n4.2 Dynamic 3D Reconstruction 4.2.1 Video Depth Evaluation Tab. 2. Video Depth Evaluation Results\n4.2.2 Camera Poses Tab. 3. Camera Poses Evaluation Results\n4.3 消融实验 (Ablation Analysis) Tab. 4. Ablation Study Results\n消融实验证明简单的 3D lifting (CoTracker3-3D baseline) 会导致 2D 追踪性能急剧下降（AJ 从 64.4 下降至 51.6）。这证明了 SyncFormer 的双分支解耦设计是有效且必要的，因为它避免了不同模态信号的纠缠。\nTab. 5. Heterogeneous Training Analysis\n实验表明，在更多、更真实的视频数据集上进行联合训练能显著提升模型在真实场景上的表现。\n5. 批判性思考 (Cirical Analysis \u0026 Personal Thoughts) 5.1 优点 (Strengths) 立意高远且切中要害: 准确地指出了现有模块化 pipeline 的核心弊病，并提出了一个逻辑自洽、优雅的“大一统”解决方案。 结构设计巧妙: SyncFormer 的双分支解耦设计和循环内的 differentiable BA，是解决 2D/3D 联合追踪问题的非常聪明的方案。 工程实践强大: 成功地在17个异构数据集上进行了复杂的多阶段训练，展示了强大的工程能力和模型的可扩展性，这是其取得 SOTA 性能的关键。 5.2 潜在缺点/可疑点 (Weaknesses/ Questionable Points) 复现门槛极高: 训练流程非常复杂，分为三阶段，使用了64块H20 GPU 。这对于算力有限的研究者来说，几乎无法复现或在此基础上进行改进。 对长视频的泛化能力: 论文中训练的视频长度在12-48帧之间 ，测试视频最长为300帧 。对于更长的视频（如数分钟级别），其累积误差和计算开销如何，没有深入探讨。 对 failure cases 分析不足: 尽管定性结果图很惊艳，但论文缺乏对模型典型 failure cases 的深入分析，例如在极端光照、快速运动模糊、或大面积无纹理区域下的表现。 5.3 Ideas to Borrow “分解+统一”：将一个复杂问题分解为几个更明确的子问题，然后设计一个统一框架进行联合优化的思想，值得借鉴。 异构数据训练策略：对于一个新任务如何整合多种不同监督形式的数据集来提升模型泛化能力，可以参考这个工作。 SyncFormer Architecture Pattern：在多模态或多任务学习中，当不同任务的 feature space 或更新动态不一致时，采用类似的解耦-交互的结构，可能是一个通用的有效策略。 ","wordCount":"422","inLanguage":"en","datePublished":"2025-07-29T00:00:00Z","dateModified":"2025-07-29T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://gavinsun0921.github.io/posts/fast-paper-reading-04/"},"publisher":{"@type":"Organization","name":"Gavin's Home","logo":{"@type":"ImageObject","url":"https://gavinsun0921.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gavinsun0921.github.io/ accesskey=h title="Gavin's Home (Alt + H)">Gavin's Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gavinsun0921.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://gavinsun0921.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gavinsun0921.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://gavinsun0921.github.io/about/ title=AboutMe><span>AboutMe</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy</h1><div class=post-meta><span title='2025-07-29 00:00:00 +0000 UTC'>July 29, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;422 words</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tldr aria-label=TL;DR>TL;DR</a></li><li><a href=#1-%e7%a0%94%e7%a9%b6%e5%8a%a8%e6%9c%ba-motivation aria-label="1. 研究动机 (Motivation)">1. 研究动机 (Motivation)</a><ul><li><a href=#11-%e8%83%8c%e6%99%af-background aria-label="1.1 背景 (Background)">1.1 背景 (Background)</a></li><li><a href=#12-%e7%8e%b0%e6%9c%89%e6%96%b9%e6%b3%95%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7-gaplimitation-of-existing-work aria-label="1.2 现有方法的局限性 (Gap/Limitation of Existing Work)">1.2 现有方法的局限性 (Gap/Limitation of Existing Work)</a></li><li><a href=#13-%e6%9c%ac%e6%96%87%e4%bb%b7%e5%80%bc-value-proposition aria-label="1.3 本文价值 (Value Proposition)">1.3 本文价值 (Value Proposition)</a></li></ul></li><li><a href=#2-%e8%a7%a3%e5%86%b3%e7%9a%84%e5%85%b3%e9%94%ae%e9%97%ae%e9%a2%98%e4%b8%8e%e8%b4%a1%e7%8c%ae-key-problem-solved--contribution aria-label="2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)">2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)</a><ul><li><a href=#21-%e8%a7%a3%e5%86%b3%e7%9a%84%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e9%97%ae%e9%a2%98 aria-label="2.1 解决的关键技术问题">2.1 解决的关键技术问题</a></li><li><a href=#22-%e6%a0%b8%e5%bf%83%e8%b4%a1%e7%8c%ae aria-label="2.2 核心贡献">2.2 核心贡献</a></li></ul></li><li><a href=#3-%e6%96%b9%e6%b3%95%e8%af%a6%e8%bf%b0-method aria-label="3. 方法详述 (Method)">3. 方法详述 (Method)</a><ul><li><a href=#31-front-end%e5%b0%ba%e5%ba%a6%e5%af%b9%e9%bd%90%e7%9a%84-video-depth--camera-pose-estimation aria-label="3.1 Front-end：尺度对齐的 video depth & camera pose estimation">3.1 Front-end：尺度对齐的 video depth & camera pose estimation</a></li><li><a href=#32-back-end-joint-motion-optimization aria-label="3.2 Back-end: Joint Motion Optimization">3.2 Back-end: Joint Motion Optimization</a></li></ul></li><li><a href=#4-%e5%ae%9e%e9%aa%8c%e5%88%86%e6%9e%90-experiments aria-label="4. 实验分析 (Experiments)">4. 实验分析 (Experiments)</a><ul><li><a href=#41-3d-point-tracking aria-label="4.1 3D Point Tracking">4.1 3D Point Tracking</a></li><li><a href=#42-dynamic-3d-reconstruction aria-label="4.2 Dynamic 3D Reconstruction">4.2 Dynamic 3D Reconstruction</a><ul><li><a href=#421-video-depth-evaluation aria-label="4.2.1 Video Depth Evaluation">4.2.1 Video Depth Evaluation</a></li><li><a href=#422-camera-poses aria-label="4.2.2 Camera Poses">4.2.2 Camera Poses</a></li></ul></li><li><a href=#43-%e6%b6%88%e8%9e%8d%e5%ae%9e%e9%aa%8c-ablation-analysis aria-label="4.3 消融实验 (Ablation Analysis)">4.3 消融实验 (Ablation Analysis)</a></li></ul></li><li><a href=#5-%e6%89%b9%e5%88%a4%e6%80%a7%e6%80%9d%e8%80%83-cirical-analysis--personal-thoughts aria-label="5. 批判性思考 (Cirical Analysis & Personal Thoughts)">5. 批判性思考 (Cirical Analysis & Personal Thoughts)</a><ul><li><a href=#51-%e4%bc%98%e7%82%b9-strengths aria-label="5.1 优点 (Strengths)">5.1 优点 (Strengths)</a></li><li><a href=#52-%e6%bd%9c%e5%9c%a8%e7%bc%ba%e7%82%b9%e5%8f%af%e7%96%91%e7%82%b9-weaknesses-questionable-points aria-label="5.2 潜在缺点/可疑点 (Weaknesses/ Questionable Points)">5.2 潜在缺点/可疑点 (Weaknesses/ Questionable Points)</a></li><li><a href=#53-ideas-to-borrow aria-label="5.3 Ideas to Borrow">5.3 Ideas to Borrow</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=tldr>TL;DR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>本文提出了一个 feed-forward 3D point tracking architecture，它将 video depth、camera pose 和 object motion 进行统一建模和 end-to-end 优化，并通过在 17 个异构数据集上的可扩展训练，实现了 SOTA 的 3D 追踪精度和推理速度。</p><h2 id=1-研究动机-motivation>1. 研究动机 (Motivation)<a hidden class=anchor aria-hidden=true href=#1-研究动机-motivation>#</a></h2><h3 id=11-背景-background>1.1 背景 (Background)<a hidden class=anchor aria-hidden=true href=#11-背景-background>#</a></h3><p>3D point tracking，即从弹幕视频中恢复长期的 3D trajectories，是一种通用的 dynamic scene representation，它在机器人、视频生成、3D/4D 重建等领域有巨大潜力 。</p><h3 id=12-现有方法的局限性-gaplimitation-of-existing-work>1.2 现有方法的局限性 (Gap/Limitation of Existing Work)<a hidden class=anchor aria-hidden=true href=#12-现有方法的局限性-gaplimitation-of-existing-work>#</a></h3><p>作者指出了当前方法的两大核心痛点：</p><ol><li><strong>模块化 pipeline 导致的误差累积</strong>：现有方法大多依赖于现成的视觉模型（如 optical flow 、monocular depth estimation）构建模块化的 pipeline 。这种分离式的处理方式忽略了 scene geometry、camera motion 和 object motion 三者之间内在的强关联性，导致误差在不同模块间传递和累积。</li><li><strong>训练数据限制了泛化能力</strong>：以往的 feed-forward 3D tracking models 严重依赖带有 ground-truth 3D tracks 的数据集进行监督训练。这类数据集难以大规模获取，导致模型在多样的 in-the-wild 视频上表现不佳，扩展性差。而基于优化的方法虽然效果好，但因其 per-scene optimization 的设计，推理速度很慢。</li></ol><h3 id=13-本文价值-value-proposition>1.3 本文价值 (Value Proposition)<a hidden class=anchor aria-hidden=true href=#13-本文价值-value-proposition>#</a></h3><p>本文认为必须将 <strong>scene geometry、camera motion</strong> 和 <strong>object motion</strong> 三者进行联合推理和显式解耦，并设计一个能利用多样化、弱监督数据源的框架。其价值在于，通过一个统一、可微的 end-to-end pipeline，实现一个<strong>高精度、高速度、高泛化性</strong>的通用 3D point tracker。</p><h2 id=2-解决的关键问题与贡献-key-problem-solved--contribution>2. 解决的关键问题与贡献 (Key Problem Solved & Contribution)<a hidden class=anchor aria-hidden=true href=#2-解决的关键问题与贡献-key-problem-solved--contribution>#</a></h2><h3 id=21-解决的关键技术问题>2.1 解决的关键技术问题<a hidden class=anchor aria-hidden=true href=#21-解决的关键技术问题>#</a></h3><p>如何设计一个<strong>可扩展的、feed-forward</strong> 的 3D tracking model，该模型能够<strong>显式地解耦（disentangle）并联合优化</strong> scene geometry (depth), camera ego-motion (pose) 和 object motion，从而摆脱对 ground-truth 监督的强依赖，并利用海量异构视频数据提升模型的泛化性和鲁棒性？</p><h3 id=22-核心贡献>2.2 核心贡献<a hidden class=anchor aria-hidden=true href=#22-核心贡献>#</a></h3><ol><li><strong>Unified Optimization Framework</strong>：提出了一个将 video depth, camera pose 和 pixel-wise 3D motion 分解并集成到一个 fully differentiable, end-to-end pipeline 中的新架构。</li><li><strong>SyncFormer 模块</strong>：设计了一个名为 SyncFormer 的核心模块，它采用双分支（2D & 3D）结构，通过 cross-attention 进行信息交互，有效解耦了在图像空间（2D）和相机坐标空间（3D）中进行的 trajectories 更新，同时支持在循环中进行可微的 Bundle Adjustment。</li><li><strong>Scalable Heterogeneous Training</strong>：该框架使得在17个不同类型的数据集上进行大规模联合训练成为可能，这些数据集的监督形式各异（如有标注的RGB-D视频、仅有位姿的视频、甚至是无标签的视频）。</li><li><strong>SOTA的性能</strong>：实验证明，该方法在 3D tracking benchmark (TAPVid-3D) 上性能相对现有方法提升超过 30%，在 dynamic reconstruction 任务上，其性能与顶尖的 optimization-based 方法相当，而推理速度快50倍。</li></ol><h2 id=3-方法详述-method>3. 方法详述 (Method)<a hidden class=anchor aria-hidden=true href=#3-方法详述-method>#</a></h2><figure class=align-center><img src=images/pipeline.jpg width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 1. SpatialTrackerV2 Pipeline Overview</p></figcaption></figure><p>SpatialTrackerV2采用了一个前后端架构的设计。</p><h3 id=31-front-end尺度对齐的-video-depth--camera-pose-estimation>3.1 Front-end：尺度对齐的 video depth & camera pose estimation<a hidden class=anchor aria-hidden=true href=#31-front-end尺度对齐的-video-depth--camera-pose-estimation>#</a></h3><p>使用 <code>Temporal Encoder</code> 来预测 consistent video depth，同时一个 <code>Neural Camera Tracker</code> 得到 coarse camera（包括 pose, scale, shift）。</p><div>$$\mathcal{P}^{t},a,b = \mathcal{H}(\mathbf{P}_{tok}, \mathbf{S}\_{tok}) \tag{1}$$</div><h3 id=32-back-end-joint-motion-optimization>3.2 Back-end: Joint Motion Optimization<a hidden class=anchor aria-hidden=true href=#32-back-end-joint-motion-optimization>#</a></h3><figure class=align-center><img src=images/SyncFormer.jpg width=80% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Fig. 2. SyncFormer</p></figcaption></figure><p><strong>核心组件：SyncFormer</strong>，一个迭代式的 Transformer module。用来联合优化估计 2D trajectories <code>$\mathcal{T}^{2d} \in \mathbb{R}^{T \times N \times 2}$</code> in UV space 以及 3D trajectories <code>$\mathcal{T}^{3d} \in \mathbb{R}^{T \times N \times 3}$</code> in the camera coordinate system。同时对每一个 trajectory 它还动态估计 visibility probability <code>$p^{vis}$</code> 和 dynamic probability <code>$p^{dyn}$</code>。</p><div>$$
\mathcal{T}^{2d}_{k+1}, \mathcal{T}^{3d}_{k+1}, p^{vis}_{k+1}, p^{vis}_{k+1} = f_{sync}(\mathcal{T}^{2d}_{k}, \mathcal{T}^{3d}_{k}, p^{vis}_{k}, p^{vis}_{k}, \mathcal{P}_k)
$$</div><p>在每次迭代中，SyncFormer同时更新 2D trajectories、3D trajectories 和 camera pose。</p><p>camera pose 通过一个可微的 Bundle Adjustment 过程进行优化，该过程利用了 2D 和 3D 轨迹之间的重投影一致性约束。</p><p>SyncFormer 关键采用了双分支（2D & 3D）解耦设计。2D 和 3D 的 Embeddings 在各自的分之内通过 self attention 处理，并通过 proxy tokens 之间的 cross attention 进行信息交换。这防止了两种不同空间（图像空间 vs. 相机空间）的更新信号相互干扰。</p><h2 id=4-实验分析-experiments>4. 实验分析 (Experiments)<a hidden class=anchor aria-hidden=true href=#4-实验分析-experiments>#</a></h2><h3 id=41-3d-point-tracking>4.1 3D Point Tracking<a hidden class=anchor aria-hidden=true href=#41-3d-point-tracking>#</a></h3><figure class=align-center><img src=images/tab1.jpg width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 1. 3D Point Tracking Results</p></figcaption></figure><h3 id=42-dynamic-3d-reconstruction>4.2 Dynamic 3D Reconstruction<a hidden class=anchor aria-hidden=true href=#42-dynamic-3d-reconstruction>#</a></h3><h4 id=421-video-depth-evaluation>4.2.1 Video Depth Evaluation<a hidden class=anchor aria-hidden=true href=#421-video-depth-evaluation>#</a></h4><figure class=align-center><img src=images/tab2.jpg width=100% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 2. Video Depth Evaluation Results</p></figcaption></figure><h4 id=422-camera-poses>4.2.2 Camera Poses<a hidden class=anchor aria-hidden=true href=#422-camera-poses>#</a></h4><figure class=align-center><img src=images/tab3.jpg width=60% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 3. Camera Poses Evaluation Results</p></figcaption></figure><h3 id=43-消融实验-ablation-analysis>4.3 消融实验 (Ablation Analysis)<a hidden class=anchor aria-hidden=true href=#43-消融实验-ablation-analysis>#</a></h3><figure class=align-center><img src=images/tab4.jpg width=60% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 4. Ablation Study Results</p></figcaption></figure><p>消融实验证明简单的 3D lifting (CoTracker3-3D baseline) 会导致 2D 追踪性能急剧下降（AJ 从 64.4 下降至 51.6）。这证明了 SyncFormer 的双分支解耦设计是有效且必要的，因为它避免了不同模态信号的纠缠。</p><figure class=align-center><img src=images/tab5.jpg width=60% style="display:block;margin:0 auto"><figcaption style=text-align:center><p>Tab. 5. Heterogeneous Training Analysis</p></figcaption></figure><p>实验表明，在更多、更真实的视频数据集上进行联合训练能显著提升模型在真实场景上的表现。</p><h2 id=5-批判性思考-cirical-analysis--personal-thoughts>5. 批判性思考 (Cirical Analysis & Personal Thoughts)<a hidden class=anchor aria-hidden=true href=#5-批判性思考-cirical-analysis--personal-thoughts>#</a></h2><h3 id=51-优点-strengths>5.1 优点 (Strengths)<a hidden class=anchor aria-hidden=true href=#51-优点-strengths>#</a></h3><ol><li><strong>立意高远且切中要害</strong>: 准确地指出了现有模块化 pipeline 的核心弊病，并提出了一个逻辑自洽、优雅的“大一统”解决方案。</li><li><strong>结构设计巧妙</strong>: SyncFormer 的双分支解耦设计和循环内的 differentiable BA，是解决 2D/3D 联合追踪问题的非常聪明的方案。</li><li><strong>工程实践强大</strong>: 成功地在17个异构数据集上进行了复杂的多阶段训练，展示了强大的工程能力和模型的可扩展性，这是其取得 SOTA 性能的关键。</li></ol><h3 id=52-潜在缺点可疑点-weaknesses-questionable-points>5.2 潜在缺点/可疑点 (Weaknesses/ Questionable Points)<a hidden class=anchor aria-hidden=true href=#52-潜在缺点可疑点-weaknesses-questionable-points>#</a></h3><ol><li><strong>复现门槛极高</strong>: 训练流程非常复杂，分为三阶段，使用了64块H20 GPU 。这对于算力有限的研究者来说，几乎无法复现或在此基础上进行改进。</li><li><strong>对长视频的泛化能力</strong>: 论文中训练的视频长度在12-48帧之间 ，测试视频最长为300帧 。对于更长的视频（如数分钟级别），其累积误差和计算开销如何，没有深入探讨。</li><li><strong>对 failure cases 分析不足</strong>: 尽管定性结果图很惊艳，但论文缺乏对模型典型 failure cases 的深入分析，例如在极端光照、快速运动模糊、或大面积无纹理区域下的表现。</li></ol><h3 id=53-ideas-to-borrow>5.3 Ideas to Borrow<a hidden class=anchor aria-hidden=true href=#53-ideas-to-borrow>#</a></h3><ol><li><strong>“分解+统一”</strong>：将一个复杂问题分解为几个更明确的子问题，然后设计一个统一框架进行联合优化的思想，值得借鉴。</li><li><strong>异构数据训练策略</strong>：对于一个新任务如何整合多种不同监督形式的数据集来提升模型泛化能力，可以参考这个工作。</li><li><strong>SyncFormer Architecture Pattern</strong>：在多模态或多任务学习中，当不同任务的 feature space 或更新动态不一致时，采用类似的解耦-交互的结构，可能是一个通用的有效策略。</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://gavinsun0921.github.io/tags/point-tracking/>Point Tracking</a></li><li><a href=https://gavinsun0921.github.io/tags/4d-reconstruction/>4D Reconstruction</a></li><li><a href=https://gavinsun0921.github.io/tags/computer-vision/>Computer Vision</a></li></ul><nav class=paginav><a class=next href=https://gavinsun0921.github.io/posts/neovim01/><span class=title>Next »</span><br><span>Neovim简单C++开发环境配置过程</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy on x" href="https://x.com/intent/tweet/?text=%5bICCV%2725%5d%20SpatialTrackerV2%3a%203D%20Point%20Tracking%20Made%20Easy&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-04%2f&amp;hashtags=PointTracking%2c4DReconstruction%2cComputerVision"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-04%2f&amp;title=%5bICCV%2725%5d%20SpatialTrackerV2%3a%203D%20Point%20Tracking%20Made%20Easy&amp;summary=%5bICCV%2725%5d%20SpatialTrackerV2%3a%203D%20Point%20Tracking%20Made%20Easy&amp;source=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-04%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-04%2f&title=%5bICCV%2725%5d%20SpatialTrackerV2%3a%203D%20Point%20Tracking%20Made%20Easy"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-04%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy on whatsapp" href="https://api.whatsapp.com/send?text=%5bICCV%2725%5d%20SpatialTrackerV2%3a%203D%20Point%20Tracking%20Made%20Easy%20-%20https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-04%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy on telegram" href="https://telegram.me/share/url?text=%5bICCV%2725%5d%20SpatialTrackerV2%3a%203D%20Point%20Tracking%20Made%20Easy&amp;url=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-04%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [ICCV'25] SpatialTrackerV2: 3D Point Tracking Made Easy on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bICCV%2725%5d%20SpatialTrackerV2%3a%203D%20Point%20Tracking%20Made%20Easy&u=https%3a%2f%2fgavinsun0921.github.io%2fposts%2ffast-paper-reading-04%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=GavinSun0921/gavinsun0921.github.io data-repo-id=R_kgDOJgiWSg data-category=Announcements data-category-id=DIC_kwDOJgiWSs4CtrHs data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://gavinsun0921.github.io/>Gavin's Home</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>